{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd73003",
   "metadata": {},
   "source": [
    "# Spatio-temporal mineral prospectivity modelling of the Papua New Guinea and Solomon Islands region\n",
    "\n",
    "### Ehsan Farahbakhsh<sup>1</sup>, Sabin Zahirovic<sup>1</sup>, Brent I. A. McInnes<sup>2</sup>, Sara Polanco<sup>1</sup>, Fabian Kohlmann<sup>3</sup>, Maria Seton<sup>1</sup>, R. Dietmar M&uuml;ller<sup>1</sup>\n",
    "\n",
    "<sup>1</sup>*EarthByte Group, School of Geosciences, The University of Sydney, Sydney, Australia*\n",
    "\n",
    "<sup>2</sup>*John de Laeter Centre, Faculty of Science and Engineering, Curtin University, Perth, Australia*\n",
    "\n",
    "<sup>3</sup>*Lithodat Pty. Ltd., Melbourne, Australia*\n",
    "\n",
    "This notebook enables the user to create a spatio-temporal mineral prospectivity model of the Papua New Guinea and Solomon Islands region using two plate motion models developed by M&uuml;ller et al. (2016) and M&uuml;ller et al. (2019). It comprises two main sections; in the first section, kinematic features are extracted, and in the second section, machine learning algorithms are applied to create a prospectivity model.\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87623f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the working environment\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from collections import deque\n",
    "import contextily as cx\n",
    "import cv2\n",
    "import geopandas as gpd\n",
    "import gplately\n",
    "from ipywidgets import interact\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.patheffects as pe\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "from ptt.subduction_convergence import subduction_convergence_over_time\n",
    "from pulearn import BaggingPuClassifier\n",
    "import pygplates\n",
    "from scipy import ndimage, stats\n",
    "from scipy.interpolate import griddata, make_interp_spline\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import scipy.spatial\n",
    "import seaborn as sns\n",
    "import shapefile\n",
    "from shapely.geometry import LineString, Point\n",
    "import statistics\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Integer, Real\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# load parameters\n",
    "from parameters_muller2019_v27 import parameters\n",
    "# from parameters_muller2016_v27 import parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c864753a",
   "metadata": {},
   "source": [
    "### Extract Convergence Kinematic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221235cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plate_motion_model = parameters['plate_motion_model']\n",
    "\n",
    "# get start time, end time, and time step from parameters.py\n",
    "time_period = parameters['time']\n",
    "start_time = time_period['start']\n",
    "end_time = time_period['end']\n",
    "time_step = time_period['step']\n",
    "time_steps = list(range(start_time, end_time+1, time_step))\n",
    "\n",
    "conv_dir = parameters['convergence_data_dir']\n",
    "conv_prefix = parameters['convergence_data_filename_prefix']\n",
    "conv_ext = parameters['convergence_data_filename_ext']\n",
    "\n",
    "trench_points_features(\n",
    "    start_time,\n",
    "    end_time,\n",
    "    time_step,\n",
    "    conv_dir,\n",
    "    conv_prefix,\n",
    "    conv_ext,\n",
    "    plate_motion_model,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c90744",
   "metadata": {},
   "source": [
    "### Create the Plate Reconstruction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plate_motion_model == 'muller2016':\n",
    "    rotation_files = parameters['rotation_file']\n",
    "    topology_files = parameters['topology_file']\n",
    "elif plate_motion_model == 'muller2019':\n",
    "    rotation_files = [os.path.join(dirpath, f) for (dirpath, dirnames, filenames) in os.walk(parameters['rotation_dir']) for f in filenames]\n",
    "    topology_files = [os.path.join(dirpath, f) for (dirpath, dirnames, filenames) in os.walk(parameters['topology_dir']) for f in filenames]\n",
    "\n",
    "coastlines = parameters['coastlines_file']\n",
    "static_polygons = parameters['static_polygons_file']\n",
    "continents = parameters['coastlines_file']\n",
    "cob = parameters['cob_file']\n",
    "\n",
    "rotation_model = pygplates.RotationModel(rotation_files)\n",
    "\n",
    "topology_features = pygplates.FeatureCollection()\n",
    "for topology_file in topology_files:\n",
    "    topology_features.add(pygplates.FeatureCollection(topology_file))\n",
    "\n",
    "# use the PlateReconstruction object to create a plate motion model\n",
    "model = gplately.PlateReconstruction(rotation_model, topology_features, static_polygons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74917245",
   "metadata": {},
   "source": [
    "### Plot Kinematic Features in a Global Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ec89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = parameters['selected_features']\n",
    "selected_features_plot = selected_features.copy()\n",
    "selected_features_plot.remove('distance_deg')\n",
    "\n",
    "agegrid_dir = parameters['agegrid_dir']\n",
    "\n",
    "extent_globe = [-180, 180, -90, 90]\n",
    "\n",
    "@interact\n",
    "def show_map(time=time_steps, feature=selected_features_plot):\n",
    "    # call the PlotTopologies object\n",
    "    gplot = gplately.PlotTopologies(model, coastlines, continents, cob, time=time)\n",
    "    \n",
    "    if plate_motion_model == 'muller2016':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2016_AREPS_v1.17_AgeGrid-{time}.nc'\n",
    "    elif plate_motion_model == 'muller2019':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2019_Tectonics_v2.0_AgeGrid-{time}.nc'\n",
    "    \n",
    "    agegrid = gplately.grids.read_netcdf_grid(agegrid_file)\n",
    "    \n",
    "    features_t = pd.read_csv(f'{conv_dir}/{conv_prefix}_{time}.00.{conv_ext}', index_col=False)\n",
    "\n",
    "    # dual colour bars\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = GridSpec(2, 2, hspace=-0.4, wspace=0.4, height_ratios=[1, 0.02])\n",
    "    ax = fig.add_subplot(gs[0, :], projection=ccrs.Mollweide(central_longitude=150))\n",
    "    \n",
    "    ax.set_global()\n",
    "\n",
    "    im = gplot.plot_grid(ax, agegrid.data, cmap='viridis', vmin=0, vmax=230, alpha=0.5, zorder=1)\n",
    "\n",
    "    gplot.plot_continents(ax, edgecolor='none', facecolor='tan', alpha=0.7, zorder=2)\n",
    "    gplot.plot_ridges(ax, color='red', label='Ridge', alpha=0.5, zorder=3)\n",
    "    gplot.plot_plate_motion_vectors(ax, spacingX=10, spacingY=10, normalise=True, alpha=0.1, zorder=4)\n",
    "\n",
    "    sc = ax.scatter(features_t['trench_lon'], features_t['trench_lat'], 50, marker='.',\n",
    "                    c=features_t[feature], cmap='YlOrRd', transform=ccrs.PlateCarree(), zorder=5) # cmap: Spectral_r, YlOrRd\n",
    "\n",
    "    gplot.plot_trenches(ax, color='k', alpha=0.3, zorder=6)\n",
    "    gplot.plot_subduction_teeth(ax, spacing=0.05, color='k', alpha=0.3, zorder=7)\n",
    "    \n",
    "    ax.gridlines(linestyle=':')\n",
    "    \n",
    "    cax1 = fig.add_subplot(gs[1, 0])\n",
    "    cax2 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    fig.colorbar(sc, cax=cax2, orientation='horizontal', label=feature, extend='both')\n",
    "    fig.colorbar(im, cax=cax1, orientation='horizontal', label='Seafloor age (Ma)', extend='max')\n",
    "        \n",
    "    ax.set_title(f'Subduction Zones {time} Ma')\n",
    "    ax.legend(loc='lower left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1ecfd",
   "metadata": {},
   "source": [
    "### Clip Trench Points based on the Target Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeeb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_extent_file = parameters['target_extent_file']\n",
    "target_extent_gdf = gpd.read_file(target_extent_file)\n",
    "target_extent_bounds = target_extent_gdf.bounds\n",
    "target_extent = [target_extent_bounds.loc[0]['minx'], target_extent_bounds.loc[0]['maxx'],\n",
    "                 target_extent_bounds.loc[0]['miny'], target_extent_bounds.loc[0]['maxy']]\n",
    "\n",
    "if target_extent[0] < -180:\n",
    "    target_extent[0] = -180\n",
    "if target_extent[1] > 180:\n",
    "    target_extent[1] = 180\n",
    "if target_extent[2] < -90:\n",
    "    target_extent[2] = -90\n",
    "if target_extent[3] > 90:\n",
    "    target_extent[3] = 90\n",
    "\n",
    "features_target_extent_files_lst = []\n",
    "\n",
    "for time in time_steps:\n",
    "    features_target_extent_files_lst.append(f'{conv_dir}/{conv_prefix}_target_extent_{time}.00.{conv_ext}')\n",
    "\n",
    "for features_target_extent_file in tqdm(features_target_extent_files_lst):\n",
    "    if not os.path.isfile(features_target_extent_file):\n",
    "        time = features_target_extent_files_lst.index(features_target_extent_file)\n",
    "        features_t = pd.read_csv(f'{conv_dir}/{conv_prefix}_{time}.00.{conv_ext}', index_col=False)\n",
    "        features_target_extent_lst = []\n",
    "\n",
    "        for i in range(features_t.shape[0]):\n",
    "            x = features_t.iloc[i]['trench_lon']\n",
    "            y = features_t.iloc[i]['trench_lat']\n",
    "            p = Point((x, y))\n",
    "            if p.within(target_extent_gdf.geometry[0]):\n",
    "                features_target_extent_lst.append(features_t.iloc[i].values)\n",
    "\n",
    "        features_target_extent = pd.DataFrame(np.row_stack(features_target_extent_lst), columns=features_t.columns)\n",
    "        features_target_extent.to_csv(features_target_extent_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da1411f",
   "metadata": {},
   "source": [
    "### Plot Kinematic Features based on the Target Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ax(ax, extent, interval_x, interval_y, font_size=None, stock_img=True, order=None):\n",
    "    if stock_img:\n",
    "        ax.stock_img()\n",
    "\n",
    "    ax.set_extent(extent)\n",
    "\n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                      linewidth=1, color='gray', alpha=0.5, linestyle='--', \n",
    "                      xlocs=np.arange(-180, 180, interval_x), ylocs=np.arange(-90, 90, interval_y), zorder=order)\n",
    "    gl.xlabels_top = False\n",
    "    gl.ylabels_right = False\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'color': 'gray', 'weight': 'bold', 'fontsize': font_size}\n",
    "    gl.ylabel_style = {'color': 'gray', 'weight': 'bold', 'fontsize': font_size}\n",
    "    \n",
    "proj = ccrs.LambertAzimuthalEqualArea(150, 0)\n",
    "\n",
    "@interact\n",
    "def show_map(time=time_steps, feature=selected_features_plot):\n",
    "    # call the PlotTopologies object\n",
    "    gplot = gplately.PlotTopologies(model, coastlines, continents, cob, time=time)\n",
    "    \n",
    "    if plate_motion_model == 'muller2016':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2016_AREPS_v1.17_AgeGrid-{time}.nc'\n",
    "    elif plate_motion_model == 'muller2019':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2019_Tectonics_v2.0_AgeGrid-{time}.nc'\n",
    "    \n",
    "    agegrid = gplately.grids.read_netcdf_grid(agegrid_file)\n",
    "    \n",
    "    features_t = pd.read_csv(f'{conv_dir}/{conv_prefix}_target_extent_{time}.00.{conv_ext}', index_col=False)\n",
    "\n",
    "    # dual colour bars\n",
    "    fig = plt.figure(figsize=(6, 8))\n",
    "    gs = GridSpec(2, 2, hspace=-0.1, wspace=0.1, height_ratios=[1, 0.02])\n",
    "    ax = fig.add_subplot(gs[0, :], projection=proj)\n",
    "    \n",
    "    set_ax(ax, target_extent, 15, 15, stock_img=False, order=8)\n",
    "\n",
    "    im = gplot.plot_grid(ax, agegrid.data, cmap='viridis', vmin=0, vmax=230, alpha=0.5, zorder=1)\n",
    "\n",
    "    gplot.plot_continents(ax, edgecolor='none', facecolor='tan', alpha=0.7, zorder=2)\n",
    "    gplot.plot_ridges(ax, color='red', label='Ridge', alpha=0.5, zorder=3)\n",
    "    gplot.plot_plate_motion_vectors(ax, spacingX=30, spacingY=30, normalise=False, regrid_shape=20, alpha=0.2, zorder=4)\n",
    "\n",
    "    sc = ax.scatter(features_t['trench_lon'], features_t['trench_lat'], 50, marker='.',\n",
    "                    c=features_t[feature], cmap='YlOrRd', transform=ccrs.PlateCarree(), zorder=5)\n",
    "\n",
    "    gplot.plot_trenches(ax, color='k', alpha=0.3, zorder=6)\n",
    "    gplot.plot_subduction_teeth(ax, spacing=0.03, color='k', alpha=0.3, zorder=7)\n",
    "    \n",
    "    cax1 = fig.add_subplot(gs[1, 0])\n",
    "    cax2 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    fig.colorbar(sc, cax=cax2, orientation='horizontal', label=feature, extend='both')\n",
    "    fig.colorbar(im, cax=cax1, orientation='horizontal', label='Seafloor age (Ma)', extend='max')\n",
    "    \n",
    "    ax.legend(loc='lower left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a596b",
   "metadata": {},
   "source": [
    "### Reconstruct Mineral Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occ_file = parameters['min_occ_file']\n",
    "coreg_input_dir = parameters['coreg_input_dir']\n",
    "coreg_input_files = parameters['coreg_input_files']\n",
    "min_occ_data_file = coreg_input_dir + coreg_input_files[0] # returns 'mineral_occurrences'\n",
    "\n",
    "if os.path.isfile(min_occ_data_file):\n",
    "    min_occ_data = pd.read_csv(min_occ_data_file, index_col=False)\n",
    "else:\n",
    "    # id, lon, lat, age, and plate id of mineral occurrences\n",
    "    min_occ_data = process_real_deposits(min_occ_file, start_time, end_time, time_step, plate_motion_model)\n",
    "    # save the attributes of mineral occurrences\n",
    "    min_occ_data.to_csv(min_occ_data_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12bccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = ccrs.LambertAzimuthalEqualArea(150, 0)\n",
    "\n",
    "@interact\n",
    "def show_map(time=time_steps, feature=selected_features_plot):\n",
    "    lons_lats_recon = []\n",
    "    \n",
    "    for min_occ in min_occ_data.iterrows():\n",
    "        if time == 0:\n",
    "            lons_lats_recon.append((min_occ[1]['lon'], min_occ[1]['lat']))\n",
    "        elif int(min_occ[1]['age']) < time:\n",
    "            lons_lats_recon.append((np.nan, np.nan))\n",
    "        elif int(min_occ[1]['age']) == time:\n",
    "            lons_lats_recon.append((min_occ[1]['lon_recon'], min_occ[1]['lat_recon']))\n",
    "        else:\n",
    "            lat_lon_recon = get_recon_ccords([min_occ[1]['lon']],\n",
    "                                             [min_occ[1]['lat']],\n",
    "                                             plate_motion_model='muller2019', # assign 'muller2019' or 'muller2016'\n",
    "                                             time=time)[0]\n",
    "            lons_lats_recon.append(tuple((lat_lon_recon[1], lat_lon_recon[0])))\n",
    "\n",
    "    # call the PlotTopologies object\n",
    "    gplot = gplately.PlotTopologies(model, coastlines, continents, cob, time=time)\n",
    "    \n",
    "    if plate_motion_model == 'muller2016':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2016_AREPS_v1.17_AgeGrid-{time}.nc'\n",
    "    elif plate_motion_model == 'muller2019':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2019_Tectonics_v2.0_AgeGrid-{time}.nc'\n",
    "    \n",
    "    agegrid = gplately.grids.read_netcdf_grid(agegrid_file)\n",
    "    \n",
    "    features_t = pd.read_csv(f'{conv_dir}/{conv_prefix}_{time}.00.{conv_ext}', index_col=False)\n",
    "\n",
    "    # dual colour bars\n",
    "    fig = plt.figure(figsize=(6, 8))\n",
    "    gs = GridSpec(2, 2, hspace=-0.1, wspace=0.1, height_ratios=[1, 0.02])\n",
    "    ax = fig.add_subplot(gs[0, :], projection=proj)\n",
    "    \n",
    "    set_ax(ax, target_extent, 15, 15, stock_img=False, order=9)\n",
    "\n",
    "    im = gplot.plot_grid(ax, agegrid.data, cmap='viridis', vmin=0, vmax=230, alpha=0.5, zorder=1)\n",
    "\n",
    "    gplot.plot_continents(ax, edgecolor='none', facecolor='tan', alpha=0.7, zorder=2)\n",
    "    gplot.plot_ridges(ax, color='red', label='Ridge', alpha=0.5, zorder=3)\n",
    "    gplot.plot_plate_motion_vectors(ax, spacingX=20, spacingY=20, normalise=False, regrid_shape=20, alpha=0.2, zorder=4)\n",
    "\n",
    "    sc0 = ax.scatter(features_t['trench_lon'], features_t['trench_lat'], 50, marker='.',\n",
    "                    c=features_t[feature], cmap='YlOrRd', transform=ccrs.PlateCarree(), zorder=5)\n",
    "    \n",
    "    sc1 = ax.scatter(\n",
    "        [coords[0] for coords in lons_lats_recon],\n",
    "        [coords[1] for coords in lons_lats_recon],\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        marker='X',\n",
    "        facecolor='yellow',\n",
    "        edgecolor='black',\n",
    "        s=50,\n",
    "        alpha=0.7,\n",
    "        zorder=6\n",
    "    )\n",
    "\n",
    "    gplot.plot_trenches(ax, color='k', alpha=0.3, zorder=7)\n",
    "    gplot.plot_subduction_teeth(ax, spacing=0.03, color='k', alpha=0.3, zorder=8)\n",
    "    \n",
    "    cax1 = fig.add_subplot(gs[1, 0])\n",
    "    cax2 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    fig.colorbar(sc0, cax=cax2, orientation='horizontal', label=feature, extend='both')\n",
    "    fig.colorbar(im, cax=cax1, orientation='horizontal', label='Seafloor age (Ma)', extend='max')\n",
    "    \n",
    "    ax.legend(loc='lower left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1cf702",
   "metadata": {},
   "source": [
    "### Create Buffer Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e7a72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if plate_motion_model == 'muller2016':\n",
    "    rotation_files = parameters['rotation_files']\n",
    "    topology_files = parameters['topology_files']\n",
    "elif plate_motion_model == 'muller2019':\n",
    "    rotation_files = [os.path.join(dirpath, f) for (dirpath, dirnames, filenames) in os.walk(parameters['rotation_dir']) for f in filenames]\n",
    "    topology_files = [os.path.join(dirpath, f) for (dirpath, dirnames, filenames) in os.walk(parameters['topology_dir']) for f in filenames]\n",
    "\n",
    "buffer_zones_files_lst = []\n",
    "\n",
    "for time in time_steps:\n",
    "    buffer_zones_files_lst.append(f'{coreg_input_dir}buffer_zones/buffer_zone_{time}_Ma.shp')\n",
    "\n",
    "for buffer_zone_file in tqdm(buffer_zones_files_lst):\n",
    "    if not os.path.isfile(buffer_zone_file):\n",
    "        index = buffer_zones_files_lst.index(buffer_zone_file)\n",
    "        \n",
    "        resolved_topologies = []\n",
    "        shared_boundary_sections = []\n",
    "        # use pygplates to resolve the topologies\n",
    "        pygplates.resolve_topologies(topology_files, rotation_files, resolved_topologies, time_steps[index], shared_boundary_sections)\n",
    "\n",
    "        # subduction zones\n",
    "        subduction_geoms = []\n",
    "        get_subduction_geometries(subduction_geoms, shared_boundary_sections)\n",
    "\n",
    "        _, buffer_zone = generate_buffer_zones(subduction_geoms, width=3)\n",
    "        buffer_zone.to_file(buffer_zone_file)\n",
    "        print(f'Buffer zones saved to {buffer_zone_file}')\n",
    "\n",
    "buffer_zones_lst = []\n",
    "buffer_zones_clipped_lst = []\n",
    "\n",
    "for buffer_zone_file in buffer_zones_files_lst:\n",
    "    buffer_zone = gpd.read_file(buffer_zone_file)\n",
    "    buffer_zone_clipped = buffer_zone.clip(target_extent_gdf)\n",
    "    buffer_zones_lst.append(buffer_zone)\n",
    "    buffer_zones_clipped_lst.append(buffer_zone_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2289d",
   "metadata": {},
   "source": [
    "### Plot Buffer Zones in a Global Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f545b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = ccrs.Mollweide(central_longitude=150)\n",
    "\n",
    "@interact\n",
    "def show_map(time=time_steps):\n",
    "    # call the PlotTopologies object\n",
    "    gplot = gplately.PlotTopologies(model, coastlines, continents, cob, time=time)\n",
    "    \n",
    "    if plate_motion_model == 'muller2016':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2016_AREPS_v1.17_AgeGrid-{time}.nc'\n",
    "    elif plate_motion_model == 'muller2019':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2019_Tectonics_v2.0_AgeGrid-{time}.nc'\n",
    "    \n",
    "    agegrid = gplately.grids.read_netcdf_grid(agegrid_file)\n",
    "    \n",
    "    # single colour bar\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    ax = plt.axes(projection=proj)\n",
    "    ax.set_global()\n",
    "\n",
    "    im = gplot.plot_grid(ax, agegrid.data, cmap='viridis', vmin=0, vmax=230, alpha=0.5, zorder=1)\n",
    "\n",
    "    gplot.plot_continents(ax, edgecolor='none', facecolor='tan', alpha=0.7, zorder=2)\n",
    "    gplot.plot_ridges(ax, color='red', label='Ridge', alpha=0.5, zorder=3)\n",
    "    gplot.plot_plate_motion_vectors(ax, spacingX=10, spacingY=10, normalise=True, alpha=0.1, zorder=4)\n",
    "\n",
    "    buffer_zones_lst[time_steps.index(time)].plot(\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        edgecolor='none',\n",
    "        facecolor='gray',\n",
    "        linewidth=1,\n",
    "        alpha=0.7,\n",
    "        zorder=5,\n",
    "    )\n",
    "    \n",
    "    gplot.plot_trenches(ax, color='k', alpha=1, zorder=6)\n",
    "    gplot.plot_subduction_teeth(ax, spacing=0.05, color='k', alpha=1, zorder=7)\n",
    "    \n",
    "    ax.gridlines(linestyle=':')\n",
    "    \n",
    "    fig.colorbar(im, orientation='horizontal', shrink=0.4, pad=0.05, label='Seafloor Age (Ma)', extend='max')\n",
    "    \n",
    "    ax.legend(loc='lower left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e6148",
   "metadata": {},
   "source": [
    "### Plot Buffer Zones based on the Target Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = ccrs.LambertAzimuthalEqualArea(150, 0)\n",
    "\n",
    "@interact\n",
    "def show_map(time=time_steps):\n",
    "    # call the PlotTopologies object\n",
    "    gplot = gplately.PlotTopologies(model, coastlines, continents, cob, time=time)\n",
    "    \n",
    "    if plate_motion_model == 'muller2016':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2016_AREPS_v1.17_AgeGrid-{time}.nc'\n",
    "    elif plate_motion_model == 'muller2019':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2019_Tectonics_v2.0_AgeGrid-{time}.nc'\n",
    "    \n",
    "    agegrid = gplately.grids.read_netcdf_grid(agegrid_file)\n",
    "    \n",
    "    # single colour bar\n",
    "    fig = plt.figure(figsize=(6, 8))\n",
    "    ax = plt.axes(projection=proj)\n",
    "    \n",
    "    set_ax(ax, target_extent, 15, 15, stock_img=False, order=8)\n",
    "\n",
    "    im = gplot.plot_grid(ax, agegrid.data, cmap='viridis', vmin=0, vmax=230, alpha=0.5, zorder=1)\n",
    "\n",
    "    gplot.plot_continents(ax, edgecolor='none', facecolor='tan', alpha=0.7, zorder=2)\n",
    "    gplot.plot_ridges(ax, color='red', label='Ridge', alpha=0.5, zorder=3)\n",
    "    gplot.plot_plate_motion_vectors(ax, spacingX=20, spacingY=20, normalise=False, regrid_shape=20, alpha=0.2, zorder=4)\n",
    "    \n",
    "    buffer_zones_clipped_lst[time_steps.index(time)].plot(\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        edgecolor='none',\n",
    "        facecolor='gray',\n",
    "        linewidth=1,\n",
    "        alpha=0.7,\n",
    "        zorder=5\n",
    "    )\n",
    "    \n",
    "    gplot.plot_trenches(ax, color='k', alpha=0.3, zorder=6)\n",
    "    gplot.plot_subduction_teeth(ax, spacing=0.03, color='k', alpha=0.3, zorder=7)\n",
    "    \n",
    "    fig.colorbar(im, orientation='horizontal', shrink=0.4, pad=0.05, label='Seafloor Age (Ma)', extend='max')\n",
    "    \n",
    "    ax.legend(loc='lower left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c83d3e3",
   "metadata": {},
   "source": [
    "### Generate Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data_file = coreg_input_dir + coreg_input_files[1]\n",
    "num_features = len(selected_features)\n",
    "\n",
    "if os.path.isfile(random_data_file):\n",
    "    random_data = pd.read_csv(random_data_file, index_col=False)\n",
    "    time_steps_random = random_data['age'].tolist()\n",
    "else:\n",
    "    time_steps_random, random_data = generate_random_samples(buffer_zones_clipped_lst,\n",
    "                                          start_time=start_time,\n",
    "                                          end_time=end_time,\n",
    "                                          time_step=time_step,\n",
    "                                          num_features=num_features,\n",
    "                                          num_features_factor=10,\n",
    "                                          rand_factor=20,\n",
    "                                          plate_motion_model='muller2019', # assign 'muller2019' or 'muller2016'\n",
    "                                          random_state=42\n",
    "                                         )\n",
    "    random_data.to_csv(random_data_file, index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffa0e2",
   "metadata": {},
   "source": [
    "### Plot Random Samples based on the Target Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps_random_sorted = sorted(time_steps_random)\n",
    "time_steps_random_sorted = [*set(time_steps_random_sorted)]\n",
    "\n",
    "proj = ccrs.LambertAzimuthalEqualArea(150, 0)\n",
    "\n",
    "@interact\n",
    "def show_map(time=time_steps_random_sorted):\n",
    "    # call the PlotTopologies object\n",
    "    gplot = gplately.PlotTopologies(model, coastlines, continents, cob, time=time)\n",
    "    \n",
    "    if plate_motion_model == 'muller2016':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2016_AREPS_v1.17_AgeGrid-{time}.nc'\n",
    "    elif plate_motion_model == 'muller2019':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2019_Tectonics_v2.0_AgeGrid-{time}.nc'\n",
    "    \n",
    "    agegrid = gplately.grids.read_netcdf_grid(agegrid_file)\n",
    "    \n",
    "    # single colour bar\n",
    "    fig = plt.figure(figsize=(6, 8))\n",
    "    ax = plt.axes(projection=proj)\n",
    "    \n",
    "    set_ax(ax, target_extent, 15, 15, stock_img=False, order=9)\n",
    "\n",
    "    im = gplot.plot_grid(ax, agegrid.data, cmap='viridis', vmin=0, vmax=230, alpha=0.5, zorder=1)\n",
    "\n",
    "    gplot.plot_continents(ax, edgecolor='none', facecolor='tan', alpha=0.7, zorder=2)\n",
    "    gplot.plot_ridges(ax, color='red', label='Ridge', alpha=0.5, zorder=3)\n",
    "    gplot.plot_plate_motion_vectors(ax, spacingX=20, spacingY=20, normalise=False, regrid_shape=20, alpha=0.2, zorder=4)\n",
    "    \n",
    "    buffer_zones_clipped_lst[time_steps.index(time)].plot(\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        edgecolor='none',\n",
    "        facecolor='gray',\n",
    "        linewidth=1,\n",
    "        alpha=0.7,\n",
    "        zorder=5\n",
    "    )\n",
    "    \n",
    "    random_samples = random_data.loc[random_data['age'] == time]\n",
    "    ax.scatter(\n",
    "        random_samples['lon'],\n",
    "        random_samples['lat'],\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        marker='X',\n",
    "        edgecolor='black',\n",
    "        facecolor='cyan',\n",
    "        s=50,\n",
    "        zorder=8\n",
    "    )\n",
    "    \n",
    "    gplot.plot_trenches(ax, color='k', alpha=0.3, zorder=6)\n",
    "    gplot.plot_subduction_teeth(ax, spacing=0.03, color='k', alpha=0.3, zorder=7)\n",
    "    \n",
    "    fig.colorbar(im, orientation='horizontal', shrink=0.4, pad=0.05, label='Seafloor Age (Ma)', extend='max')\n",
    "    \n",
    "    ax.legend(loc='lower left')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be7a737",
   "metadata": {},
   "source": [
    "### Generate Target Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c20a13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_points_coreg_in_files_lst = []\n",
    "mask_coords_files_lst = []\n",
    "\n",
    "for time in time_steps:\n",
    "    target_points_coreg_in_files_lst.append(coreg_input_dir + coreg_input_files[2] + f'_{time}_Ma.csv')\n",
    "    mask_coords_files_lst.append(coreg_input_dir + f'mask_{time}_Ma.csv')\n",
    "\n",
    "for target_points_file, mask_coords_file in tqdm(zip(target_points_coreg_in_files_lst, mask_coords_files_lst), total=len(target_points_coreg_in_files_lst)):\n",
    "    if not(os.path.isfile(target_points_file) and os.path.isfile(mask_coords_file)):\n",
    "        index = target_points_coreg_in_files_lst.index(target_points_file)\n",
    "\n",
    "        # generate target points\n",
    "        target_points, mask_coords, nx, ny = generate_samples(buffer_zones_clipped_lst[index], 0.5, 0.5, # dist_x and dist_y\n",
    "                                                              time_steps[index], plate_motion_model='muller2019') # assign 'muller2019' or 'muller2016'\n",
    "        # save the attributes of target points\n",
    "        target_points.to_csv(target_points_file, index=False, float_format='%.4f')\n",
    "        # save the mask\n",
    "        mask_coords.to_csv(mask_coords_file, index=False, float_format='%.4f')\n",
    "        print(f'Target points saved to {target_points_file}')\n",
    "\n",
    "target_points_coreg_in_lst = []\n",
    "mask_coords_lst = []\n",
    "\n",
    "for target_points_file, mask_coords_file in zip(target_points_coreg_in_files_lst, mask_coords_files_lst):\n",
    "    target_points_coreg_in_lst.append(pd.read_csv(target_points_file, index_col=False))\n",
    "    mask_coords_lst.append(pd.read_csv(mask_coords_file, index_col=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c5884",
   "metadata": {},
   "source": [
    "### Plot Target Points based on the Target Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = ccrs.LambertAzimuthalEqualArea(150, 0)\n",
    "\n",
    "@interact\n",
    "def show_map(time=time_steps):\n",
    "    # call the PlotTopologies object\n",
    "    gplot = gplately.PlotTopologies(model, coastlines, continents, cob, time=time)\n",
    "    \n",
    "    if plate_motion_model == 'muller2016':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2016_AREPS_v1.17_AgeGrid-{time}.nc'\n",
    "    elif plate_motion_model == 'muller2019':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2019_Tectonics_v2.0_AgeGrid-{time}.nc'\n",
    "\n",
    "    agegrid = gplately.grids.read_netcdf_grid(agegrid_file)\n",
    "    \n",
    "    # single colour bar\n",
    "    fig = plt.figure(figsize=(6, 8))\n",
    "    ax = plt.axes(projection=proj)\n",
    "    \n",
    "    set_ax(ax, target_extent, 15, 15, stock_img=False, order=9)\n",
    "\n",
    "    im = gplot.plot_grid(ax, agegrid.data, cmap='viridis', vmin=0, vmax=230, alpha=0.5, zorder=1)\n",
    "\n",
    "    gplot.plot_continents(ax, edgecolor='none', facecolor='tan', alpha=0.7, zorder=2)\n",
    "    gplot.plot_ridges(ax, color='red', label='Ridge', alpha=0.5, zorder=3)\n",
    "    gplot.plot_plate_motion_vectors(ax, spacingX=20, spacingY=20, normalise=False, regrid_shape=20, alpha=0.2, zorder=4)\n",
    "    \n",
    "    buffer_zones_clipped_lst[time_steps.index(time)].plot(\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        edgecolor='none',\n",
    "        facecolor='gray',\n",
    "        linewidth=1,\n",
    "        alpha=0.7,\n",
    "        zorder=5\n",
    "    )\n",
    "    \n",
    "    ax.scatter(\n",
    "        target_points_coreg_in_lst[time_steps.index(time)]['lon'],\n",
    "        target_points_coreg_in_lst[time_steps.index(time)]['lat'],\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        marker='.',\n",
    "        c='red',\n",
    "        s=1,\n",
    "        zorder=8\n",
    "    )\n",
    "        \n",
    "    gplot.plot_trenches(ax, color='k', alpha=0.3, zorder=6)\n",
    "    gplot.plot_subduction_teeth(ax, spacing=0.03, color='k', alpha=0.3, zorder=7)\n",
    "    \n",
    "    fig.colorbar(im, orientation='horizontal', shrink=0.4, pad=0.05, label='Seafloor Age (Ma)', extend='max')\n",
    "    \n",
    "#     plt.savefig(\n",
    "#     f'./figures/muller2019/target_points.png',\n",
    "#     bbox_inches='tight',\n",
    "#     pad_inches=0.1,\n",
    "#     dpi=150\n",
    "#     )\n",
    "\n",
    "    ax.legend(loc='lower left')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783efad6",
   "metadata": {},
   "source": [
    "# Coregistration and Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01944d11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coreg_output_dir = parameters['coreg_output_dir']\n",
    "positive_data_file = coreg_output_dir + coreg_input_files[0]\n",
    "unlabelled_data_file = coreg_output_dir + coreg_input_files[1]\n",
    "target_points_coreg_out_files_lst = []\n",
    "\n",
    "for time in time_steps:\n",
    "    target_points_coreg_out_files_lst.append(coreg_output_dir + coreg_input_files[2] + f'_{time}_Ma.csv')\n",
    "\n",
    "coregistration(\n",
    "    coreg_input_dir,\n",
    "    coreg_output_dir,\n",
    "    coreg_input_files,\n",
    "    conv_dir,\n",
    "    conv_prefix,\n",
    "    conv_ext,\n",
    "    time_steps=time_steps,\n",
    "    search_radius=3\n",
    ")\n",
    "\n",
    "positive_data = pd.read_csv(positive_data_file, index_col=False)\n",
    "unlabelled_data = pd.read_csv(unlabelled_data_file, index_col=False)\n",
    "\n",
    "target_points_coreg_out_lst = []\n",
    "for file_name in target_points_coreg_out_files_lst:\n",
    "    target_points_coreg_out_lst.append(pd.read_csv(file_name, index_col=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_input_dir = parameters['ml_input_dir']\n",
    "\n",
    "positive_data['label'] = 1\n",
    "unlabelled_data['label'] = 0\n",
    "\n",
    "positive_features = positive_data[selected_features]\n",
    "unlabelled_features = unlabelled_data[selected_features]\n",
    "features_all = pd.concat([positive_features, unlabelled_features]).reset_index(drop=True)\n",
    "\n",
    "# --------------------\n",
    "# save correlation csv file\n",
    "corr_file = ml_input_dir + 'correlation.csv'\n",
    "\n",
    "if os.path.isfile(corr_file):\n",
    "    corr = pd.read_csv(corr_file, index_col=0)\n",
    "else:\n",
    "    corr = features_all.corr(method='spearman').round(3)\n",
    "    corr.to_csv(corr_file, index=True)\n",
    "    \n",
    "# --------------------\n",
    "corr.style.background_gradient(cmap='coolwarm', axis=None).format('{:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b120db",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_names = parameters['selected_features_names_nounit']\n",
    "corr.columns = selected_features_names\n",
    "corr.index = selected_features_names\n",
    "f = plt.figure(figsize=(20, 15))\n",
    "plt.matshow(corr, fignum=f.number, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.xticks(range(corr.select_dtypes(['number']).shape[1]), corr.select_dtypes(['number']).columns, fontsize=14, rotation=90)\n",
    "plt.yticks(range(corr.select_dtypes(['number']).shape[1]), corr.select_dtypes(['number']).columns, fontsize=14)\n",
    "cb = plt.colorbar(aspect=50)\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "# plt.savefig(\n",
    "#     f'./figures/muller2019/correlation.png',\n",
    "#     bbox_inches='tight',\n",
    "#     pad_inches=0.1,\n",
    "#     dpi=150\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89240ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in corr.columns:\n",
    "    positive_corr_lst = []\n",
    "    negative_corr_lst = []\n",
    "    feature = corr[column]\n",
    "    for i in range(feature.shape[0]):\n",
    "        if feature[i] > 0.7:\n",
    "            positive_corr_lst.append(feature.index[i])\n",
    "        elif feature[i] < -0.7:\n",
    "            negative_corr_lst.append(feature.index[i])\n",
    "    positive_corr_lst.remove(column)\n",
    "    if positive_corr_lst:\n",
    "        print(f'{column} is positively correlated with', end=' ')\n",
    "        print(*positive_corr_lst, sep=', ')\n",
    "    if negative_corr_lst:\n",
    "        print(f'{column} is negatively correlated with', end=' ')\n",
    "        print(*negative_corr_lst, sep=', ')\n",
    "    if positive_corr_lst or negative_corr_lst:\n",
    "        print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3776bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_dir = parameters['augmentation_dir']\n",
    "\n",
    "if os.path.isfile(ml_input_dir + 'features_labels_original.csv'):\n",
    "    Xy_train_original_df = pd.read_csv(ml_input_dir + 'features_labels_original.csv', index_col=False)\n",
    "    features_list = Xy_train_original_df.columns.tolist()\n",
    "    features_list.remove('label')\n",
    "    print('Training data file already exists!')\n",
    "else:\n",
    "    positive_labels = positive_data[positive_data.columns[-1]]\n",
    "    unlabelled_labels = unlabelled_data[unlabelled_data.columns[-1]]\n",
    "    labels = pd.concat([positive_labels, unlabelled_labels]).reset_index(drop=True)\n",
    "    features_labels_original = pd.concat([features_all, labels], axis=1).reset_index(drop=True)\n",
    "\n",
    "    # drop highly correlated features\n",
    "    # create a correlation matrix\n",
    "    corr_matrix = features_all.corr(method='spearman').abs()\n",
    "    # select the upper triangle of the correlation matrix\n",
    "    corr_upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    # find features with the correlation greater than 0.7\n",
    "    corr_drop = [column for column in corr_upper.columns if any(corr_upper[column] > 0.7)]\n",
    "    print('List of the features removed due to high correlation with other features:', corr_drop)\n",
    "    # drop features\n",
    "    features = features_all.drop(corr_drop, axis=1)\n",
    "    features_list = features.columns.tolist()\n",
    "    features_labels = pd.concat([features, labels], axis=1).reset_index(drop=True)\n",
    "    features_labels_list = features_list.copy()\n",
    "    features_labels_list.append('label')\n",
    "\n",
    "    positive_data = features_labels[features_labels['label']==1]\n",
    "    unlabelled_data = features_labels[features_labels['label']==0]\n",
    "\n",
    "    positive_features = positive_data[positive_data.columns[:-1]]\n",
    "    unlabelled_features = unlabelled_data[unlabelled_data.columns[:-1]]\n",
    "\n",
    "    positive_labels = positive_data[positive_data.columns[-1]]\n",
    "    unlabelled_labels = unlabelled_data[unlabelled_data.columns[-1]]\n",
    "\n",
    "    # train test\n",
    "    # split positive samples into training and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(positive_features, positive_labels, train_size=0.8, random_state=42)\n",
    "    X_train = np.vstack((X_train, unlabelled_features))\n",
    "    y_train = np.vstack((y_train.values.reshape(-1, 1), unlabelled_labels.values.reshape(-1, 1)))\n",
    "    Xy_train_original = np.hstack((X_train, y_train))\n",
    "    Xy_train_original_df = pd.DataFrame(Xy_train_original, columns=features_labels_list)\n",
    "    Xy_train_original_df.to_csv(ml_input_dir + 'features_labels_original.csv', index=False)\n",
    "\n",
    "    print('\\nNumber of the features reduced from', num_features, 'to', positive_features.shape[1])\n",
    "    print('Number of the positive samples:', positive_features.shape[0])\n",
    "\n",
    "    print('\\nNumber of the training samples:', X_train.shape[0]), print('Number of the training labels:', y_train.shape[0])\n",
    "    print('Number of the testing samples:', X_test.shape[0]), print('Number of the testing labels:', y_test.shape[0])\n",
    "\n",
    "    st_scaler = StandardScaler()\n",
    "    X_train = st_scaler.fit_transform(X_train)\n",
    "    X_test = st_scaler.transform(X_test)\n",
    "    Xy_train = np.hstack((X_train, y_train))\n",
    "    Xy_test = np.hstack((X_test, np.reshape(y_test, (-1, 1))))\n",
    "    Xy_train_df = pd.DataFrame(Xy_train, columns=features_labels_list)\n",
    "    Xy_test_df = pd.DataFrame(Xy_test, columns=features_labels_list)\n",
    "\n",
    "    if plate_motion_model == 'muller2016':\n",
    "        Xy_train_df.to_csv(augmentation_dir + 'Xy_train_muller2016.csv', index=False)\n",
    "        Xy_test_df.to_csv(augmentation_dir + 'Xy_test_muller2016.csv', index=False)\n",
    "    elif plate_motion_model == 'muller2019':\n",
    "        Xy_train_df.to_csv(augmentation_dir + 'Xy_train_muller2019.csv', index=False)\n",
    "        Xy_test_df.to_csv(augmentation_dir + 'Xy_test_muller2019.csv', index=False)\n",
    "\n",
    "    # save the standard scaler model\n",
    "    with open(ml_input_dir + 'st_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(st_scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_points_ml_in_files_lst = []\n",
    "for time in time_steps:\n",
    "    target_points_ml_in_files_lst.append(ml_input_dir + coreg_input_files[2] + f'_{time}_Ma.csv')\n",
    "    \n",
    "target_points_ml_in_lst = []\n",
    "for target_points_ml_in_file in target_points_ml_in_files_lst:\n",
    "    if os.path.isfile(target_points_ml_in_file):\n",
    "        target_points_ml_in_lst.append(pd.read_csv(target_points_ml_in_file, index_col=False))\n",
    "    if not os.path.isfile(target_points_ml_in_file):\n",
    "        index = target_points_ml_in_files_lst.index(target_points_ml_in_file)\n",
    "        target_points_ml_in = target_points_coreg_out_lst[index][selected_features]\n",
    "        target_points_ml_in = target_points_ml_in[target_points_ml_in.columns.intersection(features_list)]\n",
    "        \n",
    "        try:\n",
    "            target_points_ml_in = st_scaler.transform(target_points_ml_in)\n",
    "        except:\n",
    "            # load the model\n",
    "            with open(ml_input_dir + 'st_scaler', 'rb') as f:\n",
    "                st_scaler = pickle.load(f)\n",
    "            target_points_ml_in = st_scaler.transform(target_points_ml_in)\n",
    "            \n",
    "        target_points_ml_in = pd.DataFrame(target_points_ml_in, columns=features_list)\n",
    "        target_points_ml_in_lst.append(target_points_ml_in)\n",
    "        target_points_ml_in.to_csv(target_points_ml_in_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd8e96",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37751b0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "augmentation_dir = parameters['augmentation_dir']\n",
    "ml_output_dir = parameters['ml_output_dir']\n",
    "\n",
    "# n_estimators: number of trees in the foreset\n",
    "# max_features: max number of features considered for splitting a node\n",
    "# max_depth: max number of levels in each decision tree\n",
    "# min_samples_split: min number of data points placed in a node before the node is split\n",
    "# min_samples_leaf: min number of data points allowed in a leaf node\n",
    "# bootstrap: method for sampling data points (with or without replacement)\n",
    "\n",
    "model_file = ml_output_dir + 'model_muller2019.pkl'\n",
    "\n",
    "if os.path.isfile(model_file):\n",
    "    print('The model already exists!')\n",
    "    # load the model\n",
    "    with open(model_file, 'rb') as f:\n",
    "        bc_best = pickle.load(f)\n",
    "    Xy_train_df = pd.read_csv(ml_output_dir + 'Xy_train_muller2019.csv', index_col=False)\n",
    "    \n",
    "    Xy_test = pd.read_csv(augmentation_dir + 'Xy_test_muller2019.csv', index_col=False)\n",
    "    X_test = Xy_test[Xy_test.columns[:-1]]\n",
    "    y_test = Xy_test[Xy_test.columns[-1]]\n",
    "    X_pred = bc_best.predict(X_test)\n",
    "    X_pred_acc = accuracy_score(y_test, X_pred)\n",
    "    print('The accuracy (testing set):', X_pred_acc)\n",
    "    \n",
    "    estimators = bc_best.estimators_\n",
    "    importances = [estimators[j].feature_importances_.reshape(-1, 1) for j in range(len(estimators))]\n",
    "    importances = np.hstack(importances)\n",
    "    \n",
    "    print(bc_best)\n",
    "else:\n",
    "    Xy_test = pd.read_csv(augmentation_dir + 'Xy_test_muller2019.csv', index_col=False)\n",
    "    X_test = Xy_test[Xy_test.columns[:-1]]\n",
    "    y_test = Xy_test[Xy_test.columns[-1]]\n",
    "    \n",
    "    # Random Forest model structure\n",
    "    rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "    # bc = BaggingPuClassifier(rf, max_samples=int(sum(y_train)), n_jobs=-1, random_state=1)\n",
    "    bc = BaggingPuClassifier(rf, n_jobs=-1, random_state=42)\n",
    "\n",
    "    n_iter = 10\n",
    "    n_fold = 10\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        print('--------------------')\n",
    "        print(f'Iteration {i+1}')\n",
    "        print('--------------------')\n",
    "\n",
    "        smote_gan_file = f'./augmentation/smote_gan_muller2019_{i+1}.csv'\n",
    "        smote_gan = pd.read_csv(smote_gan_file, index_col=False)\n",
    "        features = smote_gan[smote_gan.columns[:-1]]\n",
    "        labels = smote_gan[smote_gan.columns[-1]]\n",
    "        X_train_, X_test_, y_train_, y_test_ = train_test_split(features, labels, train_size=0.8, random_state=42)\n",
    "        Xy_train_ = np.hstack((X_train_, y_train_.values.reshape(-1, 1)))\n",
    "        Xy_train_df_ = pd.DataFrame(Xy_train_, columns=smote_gan.columns)\n",
    "        y_train_.replace(2, 1, inplace=True)\n",
    "        y_test_.replace(2, 1, inplace=True)\n",
    "\n",
    "        search_space = {\n",
    "        'base_estimator__bootstrap': Categorical([True, False]), # values for boostrap can be either True or False\n",
    "        'base_estimator__max_depth': Integer(5, 20), # values of max_depth are integers\n",
    "        'base_estimator__max_features': Categorical([None, 'sqrt','log2']), \n",
    "        'base_estimator__min_samples_leaf': Integer(2, 20),\n",
    "        'base_estimator__min_samples_split': Integer(2, 30),\n",
    "        'base_estimator__n_estimators': Integer(10, 200),\n",
    "        'max_samples': Integer(int(0.5*(len(y_train_)-sum(y_train_))), int(0.9*(len(y_train_)-sum(y_train_))))\n",
    "        }\n",
    "\n",
    "        bc_bayes_search = BayesSearchCV(bc, search_space, n_iter=50, # specify how many iterations\n",
    "                                        scoring='accuracy', n_jobs=-1, cv=n_fold, verbose=1, random_state=42, return_train_score=True)\n",
    "        bc_bayes_search.fit(X_train_, y_train_) # callback=on_step will print score after each iteration\n",
    "\n",
    "        X_pred = bc_bayes_search.best_estimator_.predict(X_test)\n",
    "        X_pred_acc = accuracy_score(y_test, X_pred)\n",
    "\n",
    "        X_pred_ = bc_bayes_search.best_estimator_.predict(X_test_)\n",
    "        X_pred_acc_ = accuracy_score(y_test_, X_pred_)\n",
    "\n",
    "        if X_pred_acc > acc_best:\n",
    "            acc_best = X_pred_acc\n",
    "            bc_best_acc_test = X_pred_acc.copy()\n",
    "            \n",
    "            bc_best = bc_bayes_search.best_estimator_\n",
    "            bc_best_acc_ = bc_bayes_search.best_score_\n",
    "            bc_best_acc_test_ = X_pred_acc_.copy()\n",
    "            \n",
    "            estimators = bc_best.estimators_\n",
    "            importances = [estimators[j].feature_importances_.reshape(-1, 1) for j in range(len(estimators))]\n",
    "            importances = np.hstack(importances)\n",
    "            \n",
    "            Xy_train_df_.to_csv(ml_output_dir + 'Xy_train_muller2019.csv', index=False)\n",
    "            print('\\nIteration number with the highest accuracy:', i+1)\n",
    "            print('The accuracy (testing set):', bc_best_acc_test)\n",
    "        \n",
    "    print('\\nThe highest accuracy during cross validation:', bc_best_acc)\n",
    "    print('The highest accuracy (validation set):', bc_best_acc_test_)\n",
    "    print('The highest accuracy (testing set):', bc_best_acc_test)\n",
    "    print('\\n')\n",
    "    print(bc_best)\n",
    "    \n",
    "    # save the model\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(bc_best, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486fa8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_estimated = []\n",
    "for pair in bc_best.oob_decision_function_:\n",
    "    if np.isnan(pair[0]) or pair[0] < pair[1]:\n",
    "        labels_estimated.append(2)\n",
    "    else:\n",
    "        labels_estimated.append(0)\n",
    "        \n",
    "print('Number of positive samples', labels_estimated.count(2))\n",
    "print('Number of negative samples', labels_estimated.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78326980",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_file = ml_output_dir + 'feature_importances_muller2019.csv'\n",
    "\n",
    "if os.path.isfile(feature_importances_file):\n",
    "    feature_importances = pd.read_csv(feature_importances_file, index_col=False).to_numpy().tolist()\n",
    "else:\n",
    "    output_features = Xy_train_original_df.columns.tolist()\n",
    "    output_features.remove('label')\n",
    "    output_features_index = [selected_features.index(feature) for feature in output_features]\n",
    "    selected_features_names = parameters['selected_features_names_nounit']\n",
    "    selected_features_names = [selected_features_names[i] for i in output_features_index]\n",
    "\n",
    "    importances_mean = importances.mean(axis=1)\n",
    "    importances_var = importances.var(axis=1)\n",
    "\n",
    "    feature_importances = [(feature, round(importance, 5)) for feature, importance in zip(selected_features_names, importances_mean)]\n",
    "    feature_importances = sorted(feature_importances, key=lambda x:x[1], reverse=True)\n",
    "    feature_importances_df = pd.DataFrame(feature_importances, columns=['Feature', 'Importance'])\n",
    "    feature_importances_df['Variance'] = importances_var\n",
    "    \n",
    "    feature_importances_df.to_csv(feature_importances_file, index=False)\n",
    "    feature_importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in feature_importances]\n",
    "# cumulative importance\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "\n",
    "x_values = list(range(len(feature_importances)))\n",
    "x_values = [x+1 for x in x_values]\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax1 = ax2.twinx()\n",
    "ax2.set_facecolor('whitesmoke')\n",
    "\n",
    "ax2.bar(x_values, sorted_importances, edgecolor='gray', facecolor='LightSalmon', width=1, alpha=0.5)\n",
    "ax1.plot(x_values, cumulative_importances, 'k--')\n",
    "\n",
    "plt.xlim(0.5, len(cumulative_importances)+0.5)\n",
    "\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "ax2.set_ylabel('Feature Importance')\n",
    "ax1.set_ylabel('Cumulative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a285014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print significant features above some threshold\n",
    "feature_importances.sort(key=lambda x:x[1])\n",
    "ft_imps = [x[1] for x in feature_importances]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_facecolor('whitesmoke')\n",
    "bar = ax.barh(range(len(ft_imps)), ft_imps)\n",
    "\n",
    "def gradientbars(bars, data):\n",
    "    ax = bars[0].axes\n",
    "    lim = ax.get_xlim()+ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        bar.set_zorder(1)\n",
    "        bar.set_facecolor('none')\n",
    "        bar.set_edgecolor('black')\n",
    "        x, y = bar.get_xy()\n",
    "        w, h = bar.get_width(), bar.get_height()\n",
    "        cmap = plt.get_cmap('coolwarm')\n",
    "        grad = np.atleast_2d(np.linspace(0, 1*w/max(data), 256))\n",
    "        ax.imshow(grad, extent=[x, x+w, y, y+h], aspect='auto', zorder=0, norm=mpl.colors.NoNorm(vmin=0, vmax=1), cmap=cmap, alpha=0.8)\n",
    "        manual_labels = [x[0] for x in feature_importances]\n",
    "        ax.set_yticks(np.arange(0, len(data), 1).tolist())\n",
    "        ax.set_yticklabels(manual_labels, minor=False)\n",
    "    ax.axis(lim)\n",
    "    ax.set_xlabel('Feature Importance')\n",
    "\n",
    "gradientbars(bar, ft_imps)\n",
    "plt.gca().yaxis.grid(False)\n",
    "\n",
    "# plt.savefig(\n",
    "#     f'./figures/muller2019/importances.png',\n",
    "#     bbox_inches='tight',\n",
    "#     pad_inches=0.1,\n",
    "#     dpi=150\n",
    "#     )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d852f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xy_train_df = pd.read_csv(ml_output_dir + 'Xy_train_muller2019.csv', index_col=False)\n",
    "\n",
    "for i in range(len(labels_estimated)):\n",
    "    if Xy_train_df['label'][i] == 1:\n",
    "        labels_estimated[i] = 1\n",
    "\n",
    "Xy_train_postpul = Xy_train_df.copy()\n",
    "Xy_train_postpul['label'] = labels_estimated\n",
    "\n",
    "@interact\n",
    "def show_map(feature=Xy_train_postpul.columns):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.set_facecolor('whitesmoke')\n",
    "    ax2 = ax1.twiny()\n",
    "    ax3 = ax2.twinx()\n",
    "    \n",
    "    ax1.hist(Xy_train_original_df[feature], bins=25, alpha=0.0)\n",
    "\n",
    "    h1 = ax2.hist(Xy_train_postpul.loc[Xy_train_postpul['label']==1][feature], bins=5, color='mediumseagreen', label='Positive', alpha=0.4, zorder=3)\n",
    "    h2 = ax2.hist(Xy_train_postpul.loc[Xy_train_postpul['label']==2][feature], bins=10, color='lightsalmon', label='Synthetic positive', alpha=0.8, zorder=2)\n",
    "    h3 = ax2.hist(Xy_train_postpul.loc[Xy_train_postpul['label']==0][feature], bins=30, color='silver', label='Negative', zorder= 1)\n",
    "    \n",
    "    kde_x = np.linspace(Xy_train_postpul[feature].min(), Xy_train_postpul[feature].max(), 100)\n",
    "    \n",
    "    kde1 = stats.gaussian_kde(Xy_train_postpul.loc[Xy_train_postpul['label']==0][feature])\n",
    "    kde2 = stats.gaussian_kde(Xy_train_postpul.loc[Xy_train_postpul['label']==2][feature])\n",
    "    kde3 = stats.gaussian_kde(Xy_train_postpul.loc[Xy_train_postpul['label']==1][feature])\n",
    "    \n",
    "    k1 = ax3.plot(kde_x, kde1(kde_x), color='grey')\n",
    "    k2 = ax3.plot(kde_x, kde2(kde_x), color='darkorange')\n",
    "    k3 = ax3.plot(kde_x, kde3(kde_x), color='green')\n",
    "\n",
    "    ax2.legend(loc='best')\n",
    "\n",
    "    ax1.set_xlabel('Total deep-sea sediment thickness (m)')\n",
    "    ax2.set_xlabel('Total deep-sea sediment thickness' + ' (standardised)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax3.set_ylabel('Probability density')\n",
    "    \n",
    "    ax2.set_ylim(0, 38)\n",
    "    ax2.set_yticks(np.arange(0, 38, 5))\n",
    "    ax3.set_ylim(0, 2.4)\n",
    "\n",
    "#     plt.savefig(\n",
    "#         f'./figures/muller2019/total_sediment_thick_m.png',\n",
    "#         bbox_inches='tight',\n",
    "#         pad_inches=0.1,\n",
    "#         dpi=150\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5cf140",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selected_features_modified = Xy_train_postpul.columns.drop('label')\n",
    "# push columns by one\n",
    "selected_features_modified_1 = deque(selected_features_modified)\n",
    "selected_features_modified_1.rotate()\n",
    "selected_features_modified_2 = deque(selected_features_modified_1)\n",
    "selected_features_modified_2.rotate()\n",
    "\n",
    "@interact\n",
    "def show_map(feature1=selected_features_modified, feature2=selected_features_modified_1, feature3=selected_features_modified_2):\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    gs = GridSpec(2, 2, hspace=0.4, wspace=0.2, height_ratios=[1, 0.03])\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax2 = ax1.twinx()\n",
    "    ax3 = ax2.twiny()\n",
    "    \n",
    "    min1 = Xy_train_original_df[feature1].min()\n",
    "    max1 = Xy_train_original_df[feature1].max()\n",
    "    min2 = Xy_train_original_df[feature2].min()\n",
    "    max2 = Xy_train_original_df[feature2].max()\n",
    "\n",
    "    x_range = np.linspace(min1, max1+0.1*max1, num=100)\n",
    "    y_range = np.linspace(min2, max2+0.1*max2, num=100)\n",
    "    grid_x, grid_y = np.meshgrid(x_range, y_range)\n",
    "    \n",
    "    grid_data = griddata(list(zip(Xy_train_original_df[feature1], Xy_train_original_df[feature2])), Xy_train_original_df[feature3],\n",
    "                         (grid_x, grid_y), method='nearest', fill_value=0)\n",
    "    grid_data = ndimage.gaussian_filter(grid_data, sigma=3)\n",
    "    cb1 = ax1.imshow(grid_data.T, extent=(min1, max1, min2, max2), origin='lower', aspect='auto', cmap=plt.cm.Spectral_r, alpha=0)\n",
    "\n",
    "    sc1 = ax1.scatter(Xy_train_original_df.loc[Xy_train_original_df['label']==0, feature1],\n",
    "                      Xy_train_original_df.loc[Xy_train_original_df['label']==0, feature2], 40, marker='.', c='blue', alpha=0)\n",
    "    sc2 = ax1.scatter(Xy_train_original_df.loc[Xy_train_original_df['label']==1, feature1],\n",
    "                      Xy_train_original_df.loc[Xy_train_original_df['label']==1, feature2], 40, marker='.', c='orange', alpha=0)\n",
    "\n",
    "    min1 = Xy_train_postpul[feature1].min()\n",
    "    max1 = Xy_train_postpul[feature1].max()\n",
    "    min2 = Xy_train_postpul[feature2].min()\n",
    "    max2 = Xy_train_postpul[feature2].max()\n",
    "    \n",
    "    x_range = np.linspace(min1, max1+0.1*max1, num=100)\n",
    "    y_range = np.linspace(min2, max2+0.1*max2, num=100)\n",
    "    grid_x, grid_y = np.meshgrid(x_range, y_range)\n",
    "    \n",
    "    grid_data = griddata(list(zip(Xy_train_postpul[feature1], Xy_train_postpul[feature2])), Xy_train_postpul[feature3],\n",
    "                         (grid_x, grid_y), method='nearest', fill_value=0)\n",
    "    grid_data = ndimage.gaussian_filter(grid_data, sigma=3)\n",
    "    cb2 = ax3.imshow(grid_data.T, extent=(min1, max1, min2, max2), origin='lower', aspect='auto', cmap=plt.cm.Spectral_r, alpha=0.7)\n",
    "    \n",
    "    sc3 = ax3.scatter(Xy_train_postpul.loc[Xy_train_postpul['label']==1, feature1],\n",
    "                      Xy_train_postpul.loc[Xy_train_postpul['label']==1, feature2],\n",
    "                      100, marker='.', facecolor='mediumseagreen', edgecolor='black', zorder=3)\n",
    "    sc4 = ax3.scatter(Xy_train_postpul.loc[Xy_train_postpul['label']==2, feature1],\n",
    "                      Xy_train_postpul.loc[Xy_train_postpul['label']==2, feature2],\n",
    "                      100, marker='.', facecolor='orangered', edgecolor='black', alpha=0.7, zorder=2)\n",
    "    sc5 = ax3.scatter(Xy_train_postpul.loc[Xy_train_postpul['label']==0, feature1],\n",
    "                      Xy_train_postpul.loc[Xy_train_postpul['label']==0, feature2],\n",
    "                      100, marker='.', facecolor='silver', edgecolor='black', alpha=0.7, zorder=1)\n",
    "    \n",
    "    # conv_angle_deg\n",
    "    ax1.set_xlim(-90, 90)\n",
    "    ax3.set_xlim(-1.8, 0.8)\n",
    "    \n",
    "    ax3.legend([sc3, sc4, sc5], ['Positive', 'Synthetic positive', 'Negative'], loc='best',  borderaxespad=0.1, fontsize=8) # numpoints=1\n",
    "\n",
    "    ax1.set_xlabel('Obliquity angle of the relative motion vector (deg)')\n",
    "    ax1.set_ylabel('Length of the arc segment (deg)')\n",
    "    ax2.set_ylabel('Length of the arc segment (standardised)')\n",
    "    ax3.set_xlabel('Obliquity angle of the relative motion vector (standardised)')\n",
    "        \n",
    "    cax1 = fig.add_subplot(gs[1, 1])\n",
    "    cax2 = fig.add_subplot(gs[1, 0])\n",
    "        \n",
    "    fig.colorbar(cb2, cax=cax2, orientation='horizontal', label='Distance to the nearest\\ntrench edge(standardised)', extend='both')\n",
    "    fig.colorbar(cb1, cax=cax1, orientation='horizontal', label='Distance to the nearest\\ntrench edge (deg)', extend='both')\n",
    "    \n",
    "#     plt.savefig(\n",
    "#         f'./figures/muller2019/features_three/dist_nearest_edge_deg.png',\n",
    "#         bbox_inches='tight',\n",
    "#         pad_inches=0.1,\n",
    "#         dpi=150\n",
    "#     )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc4fb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xy_train_postpul_pivot = Xy_train_postpul.pivot(columns=['label'])\n",
    "Xy_train_original_df_pivot = Xy_train_original_df.pivot(columns=['label'])\n",
    "\n",
    "nb_groups1 = Xy_train_postpul['label'].nunique()\n",
    "nb_groups2 = Xy_train_original_df['label'].nunique()\n",
    "\n",
    "@interact\n",
    "def show_map(feature=Xy_train_postpul.columns):\n",
    "    bplot1 = [Xy_train_postpul_pivot[feature][var].dropna() for var in Xy_train_postpul_pivot[feature]]\n",
    "    bplot2 = [Xy_train_original_df_pivot[feature][var].dropna() for var in Xy_train_original_df_pivot[feature]]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(9, 6))\n",
    "    box_param = dict(whis=(5, 95), widths=0.2, patch_artist=True,\n",
    "                     flierprops=dict(marker='.', markeredgecolor='black',\n",
    "                     fillstyle=None), medianprops=dict(color='black'))\n",
    "\n",
    "    space = 0.15\n",
    "    ax1.boxplot(bplot1, positions=np.arange(nb_groups1)-space,\n",
    "                boxprops=dict(facecolor='tab:blue'), **box_param)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.boxplot(bplot2, positions=np.arange(nb_groups2)+space,\n",
    "                boxprops=dict(facecolor='tab:orange'), **box_param)\n",
    "\n",
    "    # format x ticks\n",
    "    labelsize = 12\n",
    "    ax1.set_xticks(np.arange(nb_groups1))\n",
    "#     ax1.set_xticklabels([f'{label}' for label in np.sort(Xy_train_postpul['label'].unique())])\n",
    "    ax1.set_xticklabels(['Negative', 'Positive', 'Synthetic Positive'])\n",
    "    ax1.tick_params(axis='x', labelsize=labelsize)\n",
    "\n",
    "    # format y ticks\n",
    "    yticks_fmt = dict(axis='y', labelsize=labelsize)\n",
    "    ax1.tick_params(colors='tab:blue', **yticks_fmt)\n",
    "    ax2.tick_params(colors='tab:orange', **yticks_fmt)\n",
    "\n",
    "    # format axes labels\n",
    "    label_fmt = dict(size=12, labelpad=15)\n",
    "    ax1.set_xlabel(feature, **label_fmt)\n",
    "    ax1.set_ylabel(feature + '\\n(Standardised)', color='tab:blue', **label_fmt)\n",
    "    ax2.set_ylabel(feature + '\\n(Actual)', color='tab:orange', **label_fmt)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce3523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train_postpul_pivot = Xy_train_postpul.pivot(columns=['label'])\n",
    "Xy_train_original_df_pivot = Xy_train_original_df.pivot(columns=['label'])\n",
    "\n",
    "nb_groups1 = Xy_train_postpul['label'].nunique()\n",
    "nb_groups2 = Xy_train_original_df['label'].nunique()\n",
    "\n",
    "@interact\n",
    "def show_map(feature=Xy_train_postpul.columns):\n",
    "    bplot1 = [Xy_train_postpul_pivot[feature][var].dropna() for var in Xy_train_postpul_pivot[feature]]\n",
    "    bplot2 = [Xy_train_original_df_pivot[feature][var].dropna() for var in Xy_train_original_df_pivot[feature]]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 5))\n",
    "    box_param1 = dict(whis=(5, 95), widths=0.2, patch_artist=True,\n",
    "                      flierprops=dict(marker='.', markeredgecolor='black', fillstyle=None),\n",
    "                      medianprops=dict(color='black'), boxprops=dict(facecolor='tab:blue'))\n",
    "    box_param2 = dict(whis=(5, 95), widths=0, patch_artist=True,\n",
    "                      flierprops=dict(marker='.', markeredgecolor='none', fillstyle=None),\n",
    "                      medianprops=dict(color='none'), whiskerprops=dict(color='none'),\n",
    "                      boxprops=dict(facecolor='none', edgecolor='none'))\n",
    "\n",
    "    ax1.boxplot(bplot1, positions=np.arange(nb_groups1), **box_param1)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.boxplot(bplot2, positions=np.arange(nb_groups2), **box_param2)\n",
    "\n",
    "    # format x ticks\n",
    "    labelsize = 12\n",
    "    ax1.set_xticks(np.arange(nb_groups1))\n",
    "    ax1.set_xticklabels(['Negative', 'Positive', 'Synthetic Positive'])\n",
    "    ax1.tick_params(axis='x', labelsize=labelsize)\n",
    "\n",
    "    # format y ticks\n",
    "    yticks_fmt = dict(axis='y', labelsize=labelsize)\n",
    "\n",
    "    # format axes labels\n",
    "    label_fmt = dict(size=12, labelpad=15)\n",
    "    ax1.set_xlabel(feature, **label_fmt)\n",
    "    ax1.set_ylabel(feature + '\\n(Standardised)', **label_fmt)\n",
    "    ax2.set_ylabel(feature + '\\n(Actual)', **label_fmt)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9dac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate whiskers (for outliers)\n",
    "def calculate_whiskers(data):\n",
    "    q1, q3 = np.percentile(data, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    whisker_low = q1 - 1.5 * iqr\n",
    "    whisker_high = q3 + 1.5 * iqr\n",
    "    return whisker_low, whisker_high\n",
    "\n",
    "# pivot dataframes\n",
    "Xy_train_postpul_pivot = Xy_train_postpul.pivot(columns=['label'])\n",
    "Xy_train_original_df_pivot = Xy_train_original_df.pivot(columns=['label'])\n",
    "\n",
    "# calculate the number of unique groups\n",
    "nb_groups1 = Xy_train_postpul['label'].nunique()\n",
    "nb_groups2 = Xy_train_original_df['label'].nunique()\n",
    "\n",
    "colors = ['grey', 'seagreen', 'tomato']\n",
    "\n",
    "@interact\n",
    "def show_map(feature=Xy_train_postpul.columns):\n",
    "    vplot1_data = [Xy_train_postpul_pivot[feature][var].dropna() for var in Xy_train_postpul_pivot[feature]]\n",
    "    vplot2_data = [Xy_train_original_df_pivot[feature][var].dropna() for var in Xy_train_original_df_pivot[feature]]\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_facecolor('whitesmoke')\n",
    "\n",
    "    # create violin plots\n",
    "    vplot1_parts = ax1.violinplot(vplot1_data, positions=np.arange(nb_groups1))\n",
    "    \n",
    "    for i, part in enumerate(vplot1_parts['bodies']):\n",
    "        part.set_facecolor(colors[i])\n",
    "        part.set_edgecolor(colors[i])\n",
    "        \n",
    "    vplot1_parts['cbars'].set_edgecolor('black')\n",
    "    vplot1_parts['cmins'].set_edgecolor('black')\n",
    "    vplot1_parts['cmaxes'].set_edgecolor('black')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    vplot2_parts = ax2.violinplot(vplot2_data, positions=np.arange(nb_groups2))\n",
    "    \n",
    "    for part in vplot2_parts['bodies']:\n",
    "        part.set_facecolor('none')\n",
    "        part.set_edgecolor('none')\n",
    "        \n",
    "    vplot2_parts['cbars'].set_edgecolor('none')\n",
    "    vplot2_parts['cmins'].set_edgecolor('none')\n",
    "    vplot2_parts['cmaxes'].set_edgecolor('none')\n",
    "\n",
    "    # plotting outliers for the first violin plot\n",
    "    for i, data in enumerate(vplot1_data):\n",
    "        low, high = calculate_whiskers(data)\n",
    "        outliers = data[(data > high) | (data < low)]\n",
    "        ax1.scatter([i]*len(outliers), outliers, facecolor='red', edgecolor='black', s=20, zorder=2)\n",
    "\n",
    "    # format x ticks\n",
    "    labelsize = 12\n",
    "    ax1.set_xticks(np.arange(nb_groups1))\n",
    "    ax1.set_xticklabels(['Negative', 'Positive', 'Synthetic positive'])\n",
    "\n",
    "    # format axes labels\n",
    "    ax1.set_ylabel('Obliquity angle of the overriding absolute\\nplate velocity (standardised)')\n",
    "    ax2.set_ylabel('Obliquity angle of the overriding absolute\\nplate velocity (cm/yr)')\n",
    "    \n",
    "#     plt.savefig(\n",
    "#         f'./figures/muller2019/trench_abs_angle_deg.png',\n",
    "#         bbox_inches='tight',\n",
    "#         pad_inches=0.1,\n",
    "#         dpi=150\n",
    "#     )\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c738dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_angle_deg_postpul = Xy_train_postpul[['conv_angle_deg', 'label']]\n",
    "conv_angle_deg_postpul_filtered = conv_angle_deg_postpul[(conv_angle_deg_postpul['conv_angle_deg'] <= 0.8) & (conv_angle_deg_postpul['conv_angle_deg'] >= -1.8)]\n",
    "\n",
    "conv_angle_deg_original = Xy_train_original_df[['conv_angle_deg', 'label']]\n",
    "conv_angle_deg_original_filtered = conv_angle_deg_original[(conv_angle_deg_original['conv_angle_deg'] <= 90) & (conv_angle_deg_original['conv_angle_deg'] >= -90)]\n",
    "\n",
    "# pivot dataframes\n",
    "conv_angle_deg_postpul_pivot = conv_angle_deg_postpul_filtered.pivot(columns=['label'])\n",
    "conv_angle_deg_original_pivot = conv_angle_deg_original_filtered.pivot(columns=['label'])\n",
    "\n",
    "# calculate the number of unique groups\n",
    "nb_groups1 = conv_angle_deg_postpul_filtered['label'].nunique()\n",
    "nb_groups2 = conv_angle_deg_original_filtered['label'].nunique()\n",
    "\n",
    "vplot1_data = [conv_angle_deg_postpul_pivot['conv_angle_deg'][var].dropna() for var in conv_angle_deg_postpul_pivot['conv_angle_deg']]\n",
    "vplot2_data = [conv_angle_deg_original_pivot['conv_angle_deg'][var].dropna() for var in conv_angle_deg_original_pivot['conv_angle_deg']]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_facecolor('whitesmoke')\n",
    "\n",
    "# create violin plots\n",
    "vplot1_parts = ax1.violinplot(vplot1_data, positions=np.arange(nb_groups1))\n",
    "\n",
    "for i, part in enumerate(vplot1_parts['bodies']):\n",
    "    part.set_facecolor(colors[i])\n",
    "    part.set_edgecolor(colors[i])\n",
    "\n",
    "vplot1_parts['cbars'].set_edgecolor('black')\n",
    "vplot1_parts['cmins'].set_edgecolor('black')\n",
    "vplot1_parts['cmaxes'].set_edgecolor('black')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "vplot2_parts = ax2.violinplot(vplot2_data, positions=np.arange(nb_groups2))\n",
    "\n",
    "for part in vplot2_parts['bodies']:\n",
    "    part.set_facecolor('none')\n",
    "    part.set_edgecolor('none')\n",
    "\n",
    "vplot2_parts['cbars'].set_edgecolor('none')\n",
    "vplot2_parts['cmins'].set_edgecolor('none')\n",
    "vplot2_parts['cmaxes'].set_edgecolor('none')\n",
    "\n",
    "# plotting outliers for the first violin plot\n",
    "for i, data in enumerate(vplot1_data):\n",
    "    low, high = calculate_whiskers(data)\n",
    "    outliers = data[(data > high) | (data < low)]\n",
    "    ax1.scatter([i]*len(outliers), outliers, facecolor='red', edgecolor='black', s=20, zorder=2)\n",
    "\n",
    "# format x ticks\n",
    "labelsize = 12\n",
    "ax1.set_xticks(np.arange(nb_groups1))\n",
    "ax1.set_xticklabels(['Negative', 'Positive', 'Synthetic positive'])\n",
    "\n",
    "# format axes labels\n",
    "ax1.set_ylabel('Obliquity angle of the relative motion vector\\n(standardised)')\n",
    "ax2.set_ylabel('Obliquity angle of the relative motion vector (deg)')\n",
    "\n",
    "# plt.savefig(\n",
    "#     f'./figures/muller2019/conv_angle_deg.png',\n",
    "#     bbox_inches='tight',\n",
    "#     pad_inches=0.1,\n",
    "#     dpi=150\n",
    "# )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afdeb6",
   "metadata": {},
   "source": [
    "### Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_points_prob_files_lst = []\n",
    "target_points_prob_lst = []\n",
    "\n",
    "for time in time_steps:\n",
    "    target_points_prob_files_lst.append(ml_output_dir + f'target_points_prob_{time}_Ma.csv')\n",
    "\n",
    "for i, target_points_prob_file in tqdm(enumerate(target_points_prob_files_lst), total=len(target_points_prob_files_lst)):\n",
    "    if not(os.path.isfile(target_points_prob_file)):\n",
    "        df = target_points_ml_in_lst[target_points_prob_files_lst.index(target_points_prob_file)]\n",
    "        probs = bc_best.predict_proba(df)[:, 1].reshape(-1, 1)\n",
    "\n",
    "        mm_scaler1 = MinMaxScaler()\n",
    "        probs_scaled = mm_scaler1.fit_transform(probs)\n",
    "\n",
    "        df_xy = df.copy()\n",
    "        df_xy['lon'] = target_points_coreg_out_lst[i]['lon'].to_numpy()\n",
    "        df_xy['lat'] = target_points_coreg_out_lst[i]['lat'].to_numpy()\n",
    "        df_xy['prob'] = probs_scaled\n",
    "        df_xy.to_csv(target_points_prob_file, index=False)\n",
    "        target_points_prob_lst.append(df_xy)\n",
    "        \n",
    "for target_points_prob_file in target_points_prob_files_lst:\n",
    "    target_points_prob_lst.append(pd.read_csv(target_points_prob_file, index_col=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee829648",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = ccrs.LambertAzimuthalEqualArea(150, 0)\n",
    "\n",
    "@interact\n",
    "def show_map(time=time_steps):\n",
    "    lons_lats_recon = []\n",
    "    \n",
    "    for min_occ in min_occ_data.iterrows():\n",
    "        if time == 0:\n",
    "            lons_lats_recon.append((min_occ[1]['lon'], min_occ[1]['lat']))\n",
    "        elif int(min_occ[1]['age']) < time:\n",
    "            lons_lats_recon.append((np.nan, np.nan))\n",
    "        elif int(min_occ[1]['age']) == time:\n",
    "            lons_lats_recon.append((min_occ[1]['lon_recon'], min_occ[1]['lat_recon']))\n",
    "        else:\n",
    "            lat_lon_recon = get_recon_ccords([min_occ[1]['lon']],\n",
    "                                             [min_occ[1]['lat']],\n",
    "                                             plate_motion_model='muller2019', # assign 'muller2019' or 'muller2016'\n",
    "                                             time=time)[0]\n",
    "            lons_lats_recon.append(tuple((lat_lon_recon[1], lat_lon_recon[0])))\n",
    "            \n",
    "    # call the PlotTopologies object\n",
    "    gplot = gplately.PlotTopologies(model, coastlines, continents, cob, time=time)\n",
    "    \n",
    "    if plate_motion_model == 'muller2016':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2016_AREPS_v1.17_AgeGrid-{time}.nc'\n",
    "    elif plate_motion_model == 'muller2019':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2019_Tectonics_v2.0_AgeGrid-{time}.nc'\n",
    "\n",
    "    agegrid = gplately.grids.read_netcdf_grid(agegrid_file)\n",
    "    \n",
    "    plot_x = target_points_prob_lst[time_steps.index(time)]['lon']\n",
    "    plot_y = target_points_prob_lst[time_steps.index(time)]['lat']\n",
    "    \n",
    "    # dual colour bars\n",
    "    fig = plt.figure(figsize=(6, 8))\n",
    "    gs = GridSpec(2, 2, hspace=-0.1, wspace=0.1, height_ratios=[1, 0.02])\n",
    "    ax = fig.add_subplot(gs[0, :], projection=proj)\n",
    "    \n",
    "    set_ax(ax, target_extent, 15, 15, stock_img=False, order=9)\n",
    "\n",
    "    im = gplot.plot_grid(ax, agegrid.data, cmap='viridis', vmin=0, vmax=230, alpha=0.5, zorder=1)\n",
    "\n",
    "    gplot.plot_continents(ax, edgecolor='none', facecolor='tan', alpha=0.7, zorder=2)\n",
    "    gplot.plot_ridges(ax, color='red', label='Ridge', alpha=0.5, zorder=3)\n",
    "    gplot.plot_plate_motion_vectors(ax, spacingX=20, spacingY=20, normalise=False, regrid_shape=20, alpha=0.2, zorder=4)\n",
    "    \n",
    "    sc0 = ax.scatter(\n",
    "        plot_x,\n",
    "        plot_y,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        marker='.',\n",
    "        c=target_points_prob_lst[time_steps.index(time)]['prob'],\n",
    "        s=30,\n",
    "        cmap='YlOrRd',\n",
    "        zorder=5\n",
    "    )\n",
    "    \n",
    "    sc1 = ax.scatter(\n",
    "        [coords[0] for coords in lons_lats_recon],\n",
    "        [coords[1] for coords in lons_lats_recon],\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        marker='X',\n",
    "        facecolor='yellow',\n",
    "        edgecolor='black',\n",
    "        s=50,\n",
    "        alpha=0.7,\n",
    "        zorder=6\n",
    "    )\n",
    "\n",
    "    gplot.plot_trenches(ax, color='k', alpha=0.3, zorder=7)\n",
    "    gplot.plot_subduction_teeth(ax, spacing=0.03, color='k', alpha=0.3, zorder=8)\n",
    "\n",
    "    cax1 = fig.add_subplot(gs[1, 0])\n",
    "    cax2 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    fig.colorbar(sc0, cax=cax2, orientation='horizontal', label='Mineralisation probability')\n",
    "    fig.colorbar(im, cax=cax1, orientation='horizontal', label='Seafloor age (Ma)', extend='max')\n",
    "    \n",
    "    ax.set_title(f'Porphyry Cu-Au Mineralisation Probability {time} Ma')\n",
    "    ax.legend(loc='lower left')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea26650",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_coords_lst = []\n",
    "for mask_coords_file in mask_coords_files_lst:\n",
    "    mask_coords_lst.append(pd.read_csv(mask_coords_file, index_col=False))\n",
    "\n",
    "proj = ccrs.LambertAzimuthalEqualArea(150, 0)\n",
    "\n",
    "@interact\n",
    "def show_map(time=time_steps):\n",
    "    lons_lats_recon = []\n",
    "    \n",
    "    for min_occ in min_occ_data.iterrows():\n",
    "        if time == 0:\n",
    "            lons_lats_recon.append((min_occ[1]['lon'], min_occ[1]['lat']))\n",
    "        elif int(min_occ[1]['age']) < time:\n",
    "            lons_lats_recon.append((np.nan, np.nan))\n",
    "        elif int(min_occ[1]['age']) == time:\n",
    "            lons_lats_recon.append((min_occ[1]['lon_recon'], min_occ[1]['lat_recon']))\n",
    "        else:\n",
    "            lat_lon_recon = get_recon_ccords([min_occ[1]['lon']],\n",
    "                                             [min_occ[1]['lat']],\n",
    "                                             plate_motion_model='muller2019', # assign 'muller2019' or 'muller2016'\n",
    "                                             time=time)[0]\n",
    "            lons_lats_recon.append(tuple((lat_lon_recon[1], lat_lon_recon[0])))\n",
    "    \n",
    "    # call the PlotTopologies object\n",
    "    gplot = gplately.PlotTopologies(model, coastlines, continents, cob, time=time)\n",
    "    \n",
    "    if plate_motion_model == 'muller2016':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2016_AREPS_v1.17_AgeGrid-{time}.nc'\n",
    "    elif plate_motion_model == 'muller2019':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2019_Tectonics_v2.0_AgeGrid-{time}.nc'\n",
    "\n",
    "    agegrid = gplately.grids.read_netcdf_grid(agegrid_file)\n",
    "    \n",
    "    mask_coords = mask_coords_lst[time_steps.index(time)]\n",
    "    \n",
    "    probabilities = []\n",
    "    count = 0\n",
    "            \n",
    "    for mask in mask_coords['include']:\n",
    "        if mask:\n",
    "            probabilities.append(target_points_prob_lst[time_steps.index(time)]['prob'][count])\n",
    "            count += 1\n",
    "        else:\n",
    "            probabilities.append(np.nan)\n",
    "    \n",
    "    nx = mask_coords_lst[time_steps.index(time)]['lon'].nunique()\n",
    "    ny = mask_coords_lst[time_steps.index(time)]['lat'].nunique()\n",
    "    \n",
    "    x_min = mask_coords_lst[time_steps.index(time)]['lon'].min()\n",
    "    x_max = mask_coords_lst[time_steps.index(time)]['lon'].max()\n",
    "    y_min = mask_coords_lst[time_steps.index(time)]['lat'].min()\n",
    "    y_max = mask_coords_lst[time_steps.index(time)]['lat'].max()\n",
    "    \n",
    "    probabilities_2d = np.reshape(probabilities, (ny, nx))\n",
    "    probabilities_2d_ud = np.flipud(np.reshape(probabilities, (ny, nx)))\n",
    "    \n",
    "    # dual colour bars\n",
    "    fig = plt.figure(figsize=(6, 8))\n",
    "    gs = GridSpec(2, 2, hspace=-0.1, wspace=0.1, height_ratios=[1, 0.02])\n",
    "    ax = fig.add_subplot(gs[0, :], projection=proj)\n",
    "    \n",
    "    set_ax(ax, target_extent, 15, 15, stock_img=False, order=9)\n",
    "\n",
    "    im0 = gplot.plot_grid(ax, agegrid.data, cmap='viridis', vmin=0, vmax=230, alpha=0.5, zorder=1)\n",
    "\n",
    "    gplot.plot_continents(ax, edgecolor='none', facecolor='tan', alpha=0.7, zorder=2)\n",
    "    gplot.plot_ridges(ax, color='red', label='Ridge', alpha=0.5, zorder=3)\n",
    "    gplot.plot_plate_motion_vectors(ax, spacingX=20, spacingY=20, normalise=False, regrid_shape=20, alpha=0.2, zorder=4)\n",
    "    \n",
    "    im1 = plt.imshow(\n",
    "        probabilities_2d,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        origin='lower',\n",
    "        cmap='YlOrRd',\n",
    "        interpolation='bilinear',\n",
    "        extent=(x_min, x_max, y_min, y_max),\n",
    "        zorder=5\n",
    "    )\n",
    "    \n",
    "    sc1 = ax.scatter(\n",
    "        [coords[0] for coords in lons_lats_recon],\n",
    "        [coords[1] for coords in lons_lats_recon],\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        marker='X',\n",
    "        facecolor='yellow',\n",
    "        edgecolor='black',\n",
    "        s=50,\n",
    "        alpha=0.7,\n",
    "        zorder=6\n",
    "    )\n",
    "\n",
    "    gplot.plot_trenches(ax, color='k', alpha=0.3, zorder=7)\n",
    "    gplot.plot_subduction_teeth(ax, spacing=0.03, color='k', alpha=0.3, zorder=8)\n",
    "    \n",
    "    cax1 = fig.add_subplot(gs[1, 0])\n",
    "    cax2 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    fig.colorbar(im1, cax=cax2, orientation='horizontal', label='Mineralisation probability')\n",
    "    fig.colorbar(im0, cax=cax1, orientation='horizontal', label='Seafloor age (Ma)', extend='max')\n",
    "    \n",
    "    ax.set_title(f'Porphyry Cu-Au Mineralisation Probability {time} Ma')\n",
    "    ax.legend(loc='lower left')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd4ab9",
   "metadata": {},
   "source": [
    "# Coregistration of Mineral Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e4b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_occ_prob_dir = parameters['min_occ_prob_dir']\n",
    "min_occ_prob_files_lst = []\n",
    "min_occ_prob_tran_files_lst = []\n",
    "\n",
    "for index in min_occ_data['index']:\n",
    "    min_occ_prob_files_lst.append(min_occ_prob_dir + f'min_occ_features_{index}.csv')\n",
    "    min_occ_prob_tran_files_lst.append(min_occ_prob_dir + f'min_occ_features_tran_{index}.csv')\n",
    "\n",
    "coregistration_point(\n",
    "    min_occ_data,\n",
    "    conv_dir,\n",
    "    conv_prefix,\n",
    "    conv_ext,\n",
    "    min_occ_prob_dir,\n",
    "    file_prefix='min_occ_features',\n",
    "    time_steps=time_steps,\n",
    "    search_radius=3,\n",
    "    plate_motion_model='muller2019' # assign 'muller2019' or 'muller2016'\n",
    ")\n",
    "\n",
    "for min_occ_prob_file, min_occ_prob_tran_file in zip(min_occ_prob_files_lst, min_occ_prob_tran_files_lst):\n",
    "    if not os.path.isfile(min_occ_prob_tran_file):\n",
    "        min_occ_prob = pd.read_csv(min_occ_prob_file, index_col=False)\n",
    "        min_occ_prob_tran = min_occ_prob.copy()\n",
    "        probs = []\n",
    "\n",
    "        for i, row in min_occ_prob.iterrows():\n",
    "            try:\n",
    "                row_features = row[features_list]\n",
    "            except:\n",
    "                Xy_train_original_df = pd.read_csv(ml_input_dir + 'features_labels_original.csv', index_col=False)\n",
    "                features_list = Xy_train_original_df.columns.to_list()\n",
    "                features_list.remove('label')\n",
    "                row_features = row[features_list]\n",
    "\n",
    "            if row_features.isnull().values.any():\n",
    "                probs.append(np.nan)\n",
    "            else:\n",
    "                try:\n",
    "                    row_features = st_scaler.transform(row_features.values.reshape(1, -1))\n",
    "                except:\n",
    "                    # load the model\n",
    "                    with open(ml_input_dir + 'st_scaler.pkl', 'rb') as f:\n",
    "                        st_scaler = pickle.load(f)\n",
    "                    row_features = st_scaler.transform(row_features.values.reshape(1, -1))\n",
    "\n",
    "                min_occ_prob_tran.loc[min_occ_prob_tran['age'] == i, features_list] = row_features[0].tolist()\n",
    "                prob = bc_best.predict_proba(row_features)[0, 1]\n",
    "                probs.append(prob)\n",
    "\n",
    "        mm_scaler2 = MinMaxScaler()\n",
    "        probs_scaled = mm_scaler2.fit_transform(np.array(probs).reshape(-1, 1))\n",
    "        min_occ_prob['prob'] = probs_scaled\n",
    "        min_occ_prob_tran['prob'] = probs_scaled\n",
    "        min_occ_prob.to_csv(min_occ_prob_file, index=False)\n",
    "        min_occ_prob_tran.to_csv(min_occ_prob_tran_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aea302",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train_original_df = pd.read_csv(ml_input_dir + 'features_labels_original.csv', index_col=False)\n",
    "features_list = Xy_train_original_df.columns.to_list()\n",
    "features_list.remove('label')\n",
    "\n",
    "def smooth_1D(array, std=1):\n",
    "    return gaussian_filter1d(array, std)\n",
    "\n",
    "@interact\n",
    "def show_map(file1=min_occ_prob_files_lst, file2=min_occ_prob_tran_files_lst, feature=features_list):\n",
    "    df1 = pd.read_csv(file1, index_col=False)\n",
    "    df2 = pd.read_csv(file2, index_col=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    ax2 = fig.add_subplot(121, xlim=[df1['age'].max(), 0])\n",
    "    ax1 = ax2.twinx()\n",
    "\n",
    "    ax2.plot(df1['age'], df1['prob'], c='red')\n",
    "    \n",
    "    index1 = min_occ_prob_files_lst.index(file1)\n",
    "    age1 = min_occ_data.iloc[index1]['age']\n",
    "    ax2.vlines(x=age1, ymin=0, ymax=1, color='k', linestyles=':')\n",
    "    \n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax1.plot(df1['age'], df1[feature], c='blue')\n",
    "    \n",
    "    ax4 = fig.add_subplot(122, xlim=[df2['age'].max(), 0])\n",
    "    ax3 = ax4.twinx()\n",
    "\n",
    "    ax4.plot(df2['age'], df2['prob'], c='red')\n",
    "    \n",
    "    index2 = min_occ_prob_tran_files_lst.index(file2)\n",
    "    age2 = min_occ_data.iloc[index2]['age']\n",
    "    ax4.vlines(x=age2, ymin=0, ymax=1, color='k', linestyles=':')\n",
    "    \n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax3.plot(df2['age'], df2[feature], c='blue')\n",
    "\n",
    "    ax2.set_ylabel('Probability')\n",
    "    ax1.set_ylabel(feature + ' (Actual)')\n",
    "    ax4.set_ylabel('Probability')\n",
    "    ax3.set_ylabel(feature + ' (Standardised)')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b26913",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# smoothing function for the plots\n",
    "def smooth_data(x, y, points=200):\n",
    "    x_new = np.linspace(x.min(), x.max(), points)\n",
    "    spl = make_interp_spline(x, y, k=3)  # b-spline\n",
    "    y_smooth = spl(x_new)\n",
    "    return x_new, y_smooth\n",
    "\n",
    "@interact\n",
    "def show_map(file1=min_occ_prob_files_lst, feature=features_list):\n",
    "    df1 = pd.read_csv(file1, index_col=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    ax1 = fig.add_subplot(111, xlim=[df1['age'].max(), 0])\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    ax1.set_facecolor('whitesmoke')\n",
    "\n",
    "    # smooth and plot probability\n",
    "    age_smooth, prob_smooth = smooth_data(df1['age'], df1['prob'])\n",
    "    ln1 = ax1.plot(age_smooth, prob_smooth, c='orangered', label='Probability')\n",
    "\n",
    "    index1 = min_occ_prob_files_lst.index(file1)\n",
    "    age1 = min_occ_data.iloc[index1]['age']\n",
    "    ln2 = ax1.vlines(x=age1, ymin=0, ymax=1, color='k', linestyles=':', label='Age of formation')\n",
    "\n",
    "    ax1.set_ylim(0, 1)\n",
    "\n",
    "    # smooth and plot feature\n",
    "    age_smooth, feature_smooth = smooth_data(df1['age'], df1[feature])\n",
    "    ln3 = ax2.plot(age_smooth, feature_smooth, c='royalblue', label='Feature')\n",
    "    \n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    lines = [lines1[0], lines2[0], lines1[1]]\n",
    "    labels = [labels1[0], labels2[0], labels1[1]]\n",
    "    ax1.legend(lines, labels, loc='lower left')\n",
    "\n",
    "    ax1.set_xlabel('Age (Ma)')\n",
    "    ax1.set_ylabel('Mineralisation probability')\n",
    "    ax2.set_ylabel('Seafloor spreading rate (mm/yr)')\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    \n",
    "#     plt.savefig(\n",
    "#         f'./figures/muller2019/seafloor_spread_rate_mm_yr.png',\n",
    "#         bbox_inches='tight',\n",
    "#         pad_inches=0.1,\n",
    "#         dpi=150\n",
    "#     )\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = ccrs.LambertAzimuthalEqualArea(150, 0)\n",
    "\n",
    "@interact\n",
    "def show_map(file=min_occ_prob_files_lst, time=time_steps):\n",
    "    df = pd.read_csv(file, index_col=False)\n",
    "    lons = df.loc[df['age'] >= time]['lon'].tolist()\n",
    "    lats = df.loc[df['age'] >= time]['lat'].tolist()\n",
    "    bm = df.loc[df['age'] >= time]['before_mineralisation'].tolist()\n",
    "    val = df.loc[df['age'] >= time]['valid'].tolist()\n",
    "    \n",
    "    lons_inval = []\n",
    "    lats_inval = []\n",
    "    lons_bm = []\n",
    "    lats_bm = []\n",
    "    lons_am = []\n",
    "    lats_am = []\n",
    "    \n",
    "    # colour of the last point\n",
    "    if not val[0]:\n",
    "        last_point = 'invalid'\n",
    "    elif bm[0]:\n",
    "        last_point = 'before_mineralisation'\n",
    "    else:\n",
    "        last_point = 'after_mineralisation'\n",
    "    \n",
    "    for index in range(1, len(lons)):\n",
    "        if not val[index]:\n",
    "            lons_inval.append(lons[index])\n",
    "            lats_inval.append(lats[index])\n",
    "        elif bm[index]:\n",
    "            lons_bm.append(lons[index])\n",
    "            lats_bm.append(lats[index])\n",
    "        else:\n",
    "            lons_am.append(lons[index])\n",
    "            lats_am.append(lats[index])\n",
    "        \n",
    "    # call the PlotTopologies object\n",
    "    gplot = gplately.PlotTopologies(model, coastlines, continents, cob, time=time)\n",
    "\n",
    "    if plate_motion_model == 'muller2016':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2016_AREPS_v1.17_AgeGrid-{time}.nc'\n",
    "    elif plate_motion_model == 'muller2019':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2019_Tectonics_v2.0_AgeGrid-{time}.nc'\n",
    "        \n",
    "    agegrid = gplately.grids.read_netcdf_grid(agegrid_file)\n",
    "    \n",
    "    mask_coords = mask_coords_lst[time_steps.index(time)]\n",
    "    \n",
    "    probabilities = []\n",
    "    count = 0\n",
    "            \n",
    "    for mask in mask_coords['include']:\n",
    "        if mask:\n",
    "            probabilities.append(target_points_prob_lst[time_steps.index(time)]['prob'][count])\n",
    "            count += 1\n",
    "        else:\n",
    "            probabilities.append(np.nan)\n",
    "    \n",
    "    nx = mask_coords_lst[time_steps.index(time)]['lon'].nunique()\n",
    "    ny = mask_coords_lst[time_steps.index(time)]['lat'].nunique()\n",
    "    \n",
    "    x_min = mask_coords_lst[time_steps.index(time)]['lon'].min()\n",
    "    x_max = mask_coords_lst[time_steps.index(time)]['lon'].max()\n",
    "    y_min = mask_coords_lst[time_steps.index(time)]['lat'].min()\n",
    "    y_max = mask_coords_lst[time_steps.index(time)]['lat'].max()\n",
    "    \n",
    "    probabilities_2d = np.reshape(probabilities, (ny, nx))\n",
    "    probabilities_2d_ud = np.flipud(np.reshape(probabilities, (ny, nx)))\n",
    "    \n",
    "    # single colour bar\n",
    "    fig = plt.figure(figsize=(6, 8))\n",
    "    ax = plt.axes(projection=proj)\n",
    "    \n",
    "    set_ax(ax, target_extent, 15, 15, stock_img=False, order=10)\n",
    "\n",
    "    im0 = gplot.plot_grid(ax, agegrid.data, cmap='Blues', vmin=0, vmax=230, alpha=0.7, zorder=1)\n",
    "    \n",
    "    gplot.plot_continents(ax, edgecolor='none', facecolor='tan', zorder=2)\n",
    "    gplot.plot_ridges_and_transforms(ax, color='red', alpha=0.5, zorder=3)\n",
    "    gplot.plot_plate_motion_vectors(ax, spacingX=20, spacingY=20, normalise=False, regrid_shape=20, alpha=0.2, zorder=4)\n",
    "    \n",
    "    im1 = plt.imshow(\n",
    "        probabilities_2d,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        origin='lower',\n",
    "        cmap='Spectral_r',\n",
    "        interpolation='bilinear',\n",
    "        extent=(x_min, x_max, y_min, y_max),\n",
    "        alpha=0.5,\n",
    "        zorder=5\n",
    "    )\n",
    "    \n",
    "    if last_point == 'invalid':\n",
    "        sc = ax.scatter(lons[0], lats[0], transform=ccrs.PlateCarree(), marker='*', facecolor='gray', s=20, zorder=6)\n",
    "    elif last_point == 'before_mineralisation':\n",
    "        sc = ax.scatter(lons[0], lats[0], transform=ccrs.PlateCarree(), marker='*', facecolor='black', s=20, zorder=6)\n",
    "    else:\n",
    "        sc = ax.scatter(lons[0], lats[0], transform=ccrs.PlateCarree(), marker='*', facecolor='red', s=20, zorder=6)\n",
    "    \n",
    "    sc = ax.scatter(lons_bm, lats_bm, transform=ccrs.PlateCarree(), marker='.', facecolor='black', s=20, zorder=7)\n",
    "    sc = ax.scatter(lons_am, lats_am, transform=ccrs.PlateCarree(), marker='.', facecolor='red', s=20, zorder=7)\n",
    "    sc = ax.scatter(lons_inval, lats_inval, transform=ccrs.PlateCarree(), marker='.', facecolor='gray', s=20, zorder=7)\n",
    "    \n",
    "    gplot.plot_trenches(ax, color='k', alpha=0.3, zorder=8)\n",
    "    gplot.plot_subduction_teeth(ax, spacing=0.03, color='k', alpha=0.3, zorder=9)\n",
    "    \n",
    "    fig.colorbar(im1, orientation='horizontal', shrink=0.4, pad=0.05, label='Probability')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986bb5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lines_glow(\n",
    "    ax=None,\n",
    "    n_glow_lines: int = 10,\n",
    "    diff_linewidth: float = 1.05,\n",
    "    alpha_line: float = 0.3,\n",
    "    lines=None,\n",
    ") -> None:\n",
    "    \"\"\"Add a glow effect to the lines in an axis object.\n",
    "    Each existing line is redrawn several times with increasing width and low alpha to create the glow effect.\n",
    "    \"\"\"\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    lines = ax.get_lines() if lines is None else lines\n",
    "    lines = [lines] if isinstance(lines, Line2D) else lines\n",
    "\n",
    "    alpha_value = alpha_line / n_glow_lines\n",
    "\n",
    "    for line in lines:\n",
    "\n",
    "        data = line.get_data(orig=False)\n",
    "        linewidth = line.get_linewidth()\n",
    "        \n",
    "        try:\n",
    "            step_type = line.get_drawstyle().split('-')[1]\n",
    "        except:\n",
    "            step_type = None\n",
    "\n",
    "        for n in range(1, n_glow_lines + 1):\n",
    "            if step_type:\n",
    "                glow_line, = ax.step(*data)\n",
    "            else:\n",
    "                glow_line, = ax.plot(*data)\n",
    "            glow_line.update_from(line)  # line properties are copied as seen in this solution: https://stackoverflow.com/a/54688412/3240855\n",
    "\n",
    "            glow_line.set_alpha(alpha_value)\n",
    "            glow_line.set_linewidth(linewidth + (diff_linewidth * n))\n",
    "            glow_line.is_glow_line = True  # mark the glow lines, to disregard them in the underglow function.\n",
    "\n",
    "proj = ccrs.LambertAzimuthalEqualArea(150, 0)\n",
    "\n",
    "@interact\n",
    "def show_map(file=min_occ_prob_files_lst, time=time_steps):\n",
    "    df = pd.read_csv(file, index_col=False)\n",
    "    lons = df.loc[df['age'] >= time]['lon'].tolist()\n",
    "    lats = df.loc[df['age'] >= time]['lat'].tolist()\n",
    "    bm = df.loc[df['age'] >= time]['before_mineralisation'].tolist()\n",
    "    val = df.loc[df['age'] >= time]['valid'].tolist()\n",
    "        \n",
    "    # colour of the last point\n",
    "    if not val[0]:\n",
    "        last_point = 'invalid'\n",
    "    elif bm[0]:\n",
    "        last_point = 'before_mineralisation'\n",
    "    else:\n",
    "        last_point = 'after_mineralisation'\n",
    "\n",
    "    # list of invalid lines\n",
    "    lons_inval = []\n",
    "    lats_inval = []\n",
    "    lons_inval_temp = []\n",
    "    lats_inval_temp = []\n",
    "\n",
    "    for index2 in range(len(lons)-1, -1, -1):\n",
    "        if not val[index2]:\n",
    "            lons_inval_temp.append(lons[index2])\n",
    "            lats_inval_temp.append(lats[index2])\n",
    "            if index2 == 0:\n",
    "                lons_inval.append(lons_inval_temp)\n",
    "                lats_inval.append(lats_inval_temp)\n",
    "        else:\n",
    "            if len(lons_inval_temp) != 0:\n",
    "                lons_inval_temp.append(lons[index2])\n",
    "                lats_inval_temp.append(lats[index2])\n",
    "                lons_inval.append(lons_inval_temp)\n",
    "                lats_inval.append(lats_inval_temp)\n",
    "                lons_inval_temp = []\n",
    "                lats_inval_temp = []\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # list of lines created before mineralisation\n",
    "    lons_bm = []\n",
    "    lats_bm = []\n",
    "    lons_bm_temp = []\n",
    "    lats_bm_temp = []\n",
    "\n",
    "    for index2 in range(len(lons)-1, -1, -1):\n",
    "        if bm[index2] and val[index2]:\n",
    "            lons_bm_temp.append(lons[index2])\n",
    "            lats_bm_temp.append(lats[index2])\n",
    "            if index2 == 0:\n",
    "                lons_bm.append(lons_bm_temp)\n",
    "                lats_bm.append(lats_bm_temp)\n",
    "        else:\n",
    "            if len(lons_bm_temp) != 0:\n",
    "                lons_bm_temp.append(lons[index2])\n",
    "                lats_bm_temp.append(lats[index2])\n",
    "                lons_bm.append(lons_bm_temp)\n",
    "                lats_bm.append(lats_bm_temp)\n",
    "                lons_bm_temp = []\n",
    "                lats_bm_temp = []\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # list of lines created after mineralisation\n",
    "    lons_am = []\n",
    "    lats_am = []\n",
    "    lons_am_temp = []\n",
    "    lats_am_temp = []\n",
    "\n",
    "    for index2 in range(len(lons)-1, -1, -1):\n",
    "        if not bm[index2] and val[index2]:\n",
    "            lons_am_temp.append(lons[index2])\n",
    "            lats_am_temp.append(lats[index2])\n",
    "            if index2 == 0:\n",
    "                lons_am.append(lons_am_temp)\n",
    "                lats_am.append(lats_am_temp)\n",
    "        else:\n",
    "            if len(lons_am_temp) != 0:\n",
    "                lons_am_temp.append(lons[index2])\n",
    "                lats_am_temp.append(lats[index2])\n",
    "                lons_am.append(lons_am_temp)\n",
    "                lats_am.append(lats_am_temp)\n",
    "                lons_am_temp = []\n",
    "                lats_am_temp = []\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # call the PlotTopologies object\n",
    "    gplot = gplately.PlotTopologies(model, coastlines, continents, cob, time=time)\n",
    "    \n",
    "    if plate_motion_model == 'muller2016':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2016_AREPS_v1.17_AgeGrid-{time}.nc'\n",
    "    elif plate_motion_model == 'muller2019':\n",
    "        agegrid_file = agegrid_dir + f'Muller_etal_2019_Tectonics_v2.0_AgeGrid-{time}.nc'\n",
    "\n",
    "    agegrid = gplately.grids.read_netcdf_grid(agegrid_file)\n",
    "    \n",
    "    mask_coords = mask_coords_lst[time_steps.index(time)]\n",
    "    \n",
    "    probabilities = []\n",
    "    count = 0\n",
    "            \n",
    "    for mask in mask_coords['include']:\n",
    "        if mask:\n",
    "            probabilities.append(target_points_prob_lst[time_steps.index(time)]['prob'][count])\n",
    "            count += 1\n",
    "        else:\n",
    "            probabilities.append(np.nan)\n",
    "    \n",
    "    nx = mask_coords_lst[time_steps.index(time)]['lon'].nunique()\n",
    "    ny = mask_coords_lst[time_steps.index(time)]['lat'].nunique()\n",
    "    \n",
    "    x_min = mask_coords_lst[time_steps.index(time)]['lon'].min()\n",
    "    x_max = mask_coords_lst[time_steps.index(time)]['lon'].max()\n",
    "    y_min = mask_coords_lst[time_steps.index(time)]['lat'].min()\n",
    "    y_max = mask_coords_lst[time_steps.index(time)]['lat'].max()\n",
    "    \n",
    "    probabilities_2d = np.reshape(probabilities, (ny, nx))\n",
    "    probabilities_2d_ud = np.flipud(np.reshape(probabilities, (ny, nx)))\n",
    "    \n",
    "    # single colour bar\n",
    "    fig = plt.figure(figsize=(6, 8))\n",
    "    ax = plt.axes(projection=proj)\n",
    "    \n",
    "    set_ax(ax, target_extent, 15, 15, stock_img=False, order=10)\n",
    "\n",
    "    im0 = gplot.plot_grid(ax, agegrid.data, cmap='Blues', vmin=0, vmax=230, alpha=0.7, zorder=1)\n",
    "    \n",
    "    gplot.plot_continents(ax, edgecolor='none', facecolor='tan', zorder=2)\n",
    "    gplot.plot_ridges_and_transforms(ax, color='red', alpha=0.5, zorder=3)\n",
    "    gplot.plot_plate_motion_vectors(ax, spacingX=20, spacingY=20, normalise=False, regrid_shape=20, alpha=0.2, zorder=4)\n",
    "    \n",
    "    im1 = plt.imshow(\n",
    "        probabilities_2d,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        origin='lower',\n",
    "        cmap='Spectral_r',\n",
    "        interpolation='bilinear',\n",
    "        extent=(x_min, x_max, y_min, y_max),\n",
    "        alpha=0.5,\n",
    "        zorder=5\n",
    "    )\n",
    "        \n",
    "    for lons_, lats_ in zip(lons_bm, lats_bm):\n",
    "        sc = ax.plot(lons_, lats_, transform=ccrs.PlateCarree(), color='black', zorder=6)\n",
    "        \n",
    "    for lons_, lats_ in zip(lons_am, lats_am):\n",
    "        sc = ax.plot(lons_, lats_, transform=ccrs.PlateCarree(), color='red', zorder=6)\n",
    "        \n",
    "    for lons_, lats_ in zip(lons_inval, lats_inval):\n",
    "        sc = ax.plot(lons_, lats_, transform=ccrs.PlateCarree(), color='gray', zorder=6)\n",
    "        \n",
    "    if last_point == 'invalid':\n",
    "        sc = ax.scatter(lons[0], lats[0], transform=ccrs.PlateCarree(), marker='*', facecolor='gray', s=20, zorder=7)\n",
    "    elif last_point == 'before_mineralisation':\n",
    "        sc = ax.scatter(lons[0], lats[0], transform=ccrs.PlateCarree(), marker='*', facecolor='black', s=20, zorder=7)\n",
    "    else:\n",
    "        sc = ax.scatter(lons[0], lats[0], transform=ccrs.PlateCarree(), marker='*', facecolor='red', s=20, zorder=7)\n",
    "    \n",
    "    gplot.plot_trenches(ax, color='k', alpha=0.3, zorder=8)\n",
    "    gplot.plot_subduction_teeth(ax, spacing=0.03, color='k', alpha=0.3, zorder=9)\n",
    "    \n",
    "    fig.colorbar(im1, orientation='horizontal', shrink=0.4, pad=0.05, label='Probability')\n",
    "    \n",
    "#     make_lines_glow()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069902af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "png_jun23",
   "language": "python",
   "name": "png_jun23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
